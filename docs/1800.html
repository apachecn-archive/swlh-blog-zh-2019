<html>
<head>
<title>A Text Classification Approach using Vector Space Modelling(Doc2Vec) &amp; PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种使用向量空间模型(Doc2Vec)和PCA的文本分类方法</h1>
<blockquote>原文：<a href="https://medium.com/swlh/a-text-classification-approach-using-vector-space-modelling-doc2vec-pca-74fb6fd73760?source=collection_archive---------5-----------------------#2019-03-21">https://medium.com/swlh/a-text-classification-approach-using-vector-space-modelling-doc2vec-pca-74fb6fd73760?source=collection_archive---------5-----------------------#2019-03-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c143" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设给你很多带有正反标签的影评故事。现在，如果你必须开发一些自动算法来判断一篇新写的评论文章是正面的还是负面的，你如何用最简单的方法继续下去呢？</p><p id="2b6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">毫无疑问，你必须应用标准的ML分类技术(从问题陈述中，很容易理解这是一个分类问题)。但是这里的问题是大量的文本数据。这些不是数字，所以你怎么能让计算机/算法理解评论故事的重要性呢？让我们采取一步一步的方法</p><p id="28ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">从数据源获取数据</strong></p><p id="09d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在康乃尔大学的门户网站(【http://www.cs.cornell.edu/people/pabo/movie-review-data/】T2)可以找到电影评论故事的样本。可以下载“<a class="ae je" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz" rel="noopener ugc nofollow" target="_blank">极性数据集v2.0 </a>”。该数据集包含两个名为“pos”&amp;“neg”的文件夹，每个文件夹下有1000个文件。每个文件都有以纯文本格式编写的评论文章。您可以使用下面的Python代码片段读取和打印数据集:</p><pre class="jf jg jh ji fd jj jk jl jm aw jn bi"><span id="aa38" class="jo jp hi jk b fi jq jr l js jt">import pandas as pd<br/>import pathlib as pl</span><span id="a1ed" class="jo jp hi jk b fi ju jr l js jt">def _read_all_reviews_():<br/> all_reviews = []</span><span id="563f" class="jo jp hi jk b fi ju jr l js jt"> for p in pl.Path('../data/txt_sentoken/pos').iterdir():<br/> file = open(p, 'r')<br/> all_reviews.append({'reviews_content': file.read(), 'category': 'positive'})<br/> file.close()</span><span id="22f8" class="jo jp hi jk b fi ju jr l js jt"> for p in pl.Path('../data/txt_sentoken/neg').iterdir():<br/> file = open(p, 'r')<br/> all_reviews.append({'reviews_content': file.read(), 'category': 'negative'})<br/> file.close()</span><span id="85f1" class="jo jp hi jk b fi ju jr l js jt"> all_reviews_df = pd.DataFrame(all_reviews)<br/> return all_reviews_df</span><span id="0de6" class="jo jp hi jk b fi ju jr l js jt">print(_read_all_reviews_())</span></pre><p id="434d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">ML管道的概念</strong></p><p id="b1f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">任何ML算法执行的最佳实践都包括管道创建。然而，在不创建管道的情况下，您也可以执行ML算法。但是流水线让你的生活变得简单。我们先来了解一下‘管道’是什么意思？</p><p id="aa60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数ML算法需要对票价数据进行预处理。所有这些预处理和实际算法都可以被配置为单独的可重用步骤。所有这些步骤连接在一个具有入口和出口的单一实体中，称为“管道”。现在让我们看看在我们的问题中有哪些必要的步骤。</p><p id="e74c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">创建矢量空间模型</strong></p><p id="4283" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">管道中的第一步是将数据转换成数值，因为它目前是纯文本格式。有标准型号，如“Tf-Idf”、“Word2Vec”、“Doc2Vec”等。所有这些都被称为文本分析中的“向量空间模型”,其中每一个都只给出文本数据的数字向量表示。该向量作为文档的特征向量工作，并确定有多少个特征。我使用“Doc2Vec”作为这个问题的模型，因为每个评论都可以被视为一个单独的文档，并且“Doc2Vec”在理解文本的上下文含义方面非常有效(与Tf-Idf相比，Tf-Idf只是一个纯粹的基于频率的模型)。文档的“Doc2Vec”是通过对照随机采样的单词的邻近单词训练神经网络而生成的，反之亦然。这里，只有学习到的隐藏层的权重才是关注的问题，并被取为“Doc2Vec”值。事实上有两种技术:分布式内存(DM)和分布式单词包(DBOW)。有关更多详情，您可以查看以下资源:</p><ul class=""><li id="e51b" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><a class="ae je" href="https://rare-technologies.com/doc2vec-tutorial/" rel="noopener ugc nofollow" target="_blank">稀有科技的Doc2Vec教程</a></li><li id="46cf" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><a class="ae je" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">分布式陈述语句&amp;文档</a></li></ul><p id="df1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下Python代码片段用作为文档创建Doc2Vec的管道步骤:</p><pre class="jf jg jh ji fd jj jk jl jm aw jn bi"><span id="a39a" class="jo jp hi jk b fi jq jr l js jt">from sklearn.pipeline import Pipeline<br/>from sklearn.decomposition import PCA<br/>from gensim.models.doc2vec import TaggedDocument, Doc2Vec<br/>from sklearn.base import BaseEstimator, TransformerMixin<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/>from gensim.parsing.preprocessing import preprocess_string<br/>from sklearn import utils<br/>from tqdm import tqdm</span><span id="3855" class="jo jp hi jk b fi ju jr l js jt">class Doc2VecTransformer(BaseEstimator):</span><span id="fe18" class="jo jp hi jk b fi ju jr l js jt"> def __init__(self, vector_size=100, learning_rate=0.02, epochs=20):<br/> self.learning_rate = learning_rate<br/> self.epochs = epochs<br/> self._model = None<br/> self.vector_size = vector_size<br/> self.workers = multiprocessing.cpu_count()</span><span id="9e54" class="jo jp hi jk b fi ju jr l js jt"> def fit(self, df_x, df_y=None):<br/> tagged_x = [TaggedDocument(preprocess_string(row['reviews_content']), [index]) for index, row in df_x.iterrows()]<br/> model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)</span><span id="c677" class="jo jp hi jk b fi ju jr l js jt"> for epoch in range(self.epochs):<br/> model.train(utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)<br/> model.alpha -= self.learning_rate<br/> model.min_alpha = model.alpha</span><span id="d414" class="jo jp hi jk b fi ju jr l js jt"> self._model = model<br/> return self</span><span id="0638" class="jo jp hi jk b fi ju jr l js jt"> def transform(self, df_x):<br/>return np.asmatrix(np.array([self._model.infer_vector(preprocess_string(row['reviews_content'])) for index, row in df_x.iterrows()]))</span></pre><p id="83b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码片段中，“vector_size”参数决定了每个文档的向量长度。基本上，它是如上所述的文档的提取特征的数量。这些特征是“虚拟的性质”,在物理上很难想象。你可以说，在物理上它没有任何重要性，但是这些为数学建模增加了巨大的价值。您可以看到，该模型以调整后的学习速率反复训练了多次。只是为了更好的结果。“vector_size”和“learning_rate”可以作为超参数来调整模型。稍后我会解释。最终结果是，Doc2Vec特性作为矩阵返回，其中行是文档编号，列是特性值。</p><p id="59d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">doc 2 vec的PCA建模</strong></p><p id="9e76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您所看到的，Doc2Vec可以生成大量的特性，这取决于您在参数“vector_size”中传递的值，因此将它简化为相关的特性集总是很方便的。主成分在这里可以起到很好的作用。因此，您的下一步工作将是对Doc2Vec进行PCA转换。主要组件也是虚拟特征，如“Doc2Vec ”,难以可视化/概念化，但有利于数学建模。您可以在PCA transformer中将“n_components”作为参数(主成分的数量)传递。结果你会得到主成分分析向量。显然，你的“n_components”应该比“vector_size”小得多，可能是它的二分之一或三分之一。</p><p id="284f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">拟合逻辑回归分类器</strong></p><p id="f050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完成PCA后，您可以将PCA向量放入二元逻辑回归分类器中(因为这里的输出是分类变量，只能有两个值:“正”和“负”)，这将是您管道中的最后一步。根据PCA向量值，该分类器将预测电影评论故事是正面的还是负面的。</p><p id="58a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的python代码片段显示了整个管道:</p><pre class="jf jg jh ji fd jj jk jl jm aw jn bi"><span id="6e63" class="jo jp hi jk b fi jq jr l js jt">def train_and_build_model():<br/> all_reviews_df = _read_all_reviews_()<br/> train_x_df, test_x_df, train_y_df, test_y_df = train_test_split(all_reviews_df[['reviews_content']], all_reviews_df[['category']])</span><span id="3aca" class="jo jp hi jk b fi ju jr l js jt"> pl = Pipeline(steps=[('doc2vec', Doc2VecTransformer(vector_size=220)),<br/> ('pca', PCA(n_components=100)),<br/> ('logistic', LogisticRegression())<br/> ])<br/> pl.fit(train_x_df[['reviews_content']], train_y_df[['category']])<br/> predictions_y = pl.predict(test_x_df[['reviews_content']])<br/> print('Accuracy: ', metrics.accuracy_score(y_true=test_y_df[['category']], y_pred=predictions_y))</span></pre><p id="306e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，你可能会想我是从哪里得到“n_components”的值100和“vector_size”的值220的。你可以在这里传递任何值。一般对于Doc2Vec，“vector_size”保持在100 &amp; 300之间。但是对于主成分分析，这取决于你想要保留多少主成分来保持数据中足够的可变性。“向量大小”和“n个组件”都是这里的“超参数”。</p><p id="eec4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">用网格搜索调整超参数</strong></p><p id="2b9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Python中的网格搜索库为您提供了找到最佳超参数集的机制。您可以提供一系列可能的值，以便从分类器模型中获得最佳准确性。以下Python代码片段可以为您提供这一点:</p><pre class="jf jg jh ji fd jj jk jl jm aw jn bi"><span id="dfe2" class="jo jp hi jk b fi jq jr l js jt">from sklearn.model_selection import GridSearchCV</span><span id="5369" class="jo jp hi jk b fi ju jr l js jt">def train_long_range_grid_search():<br/> all_reviews_df = _read_all_reviews_()<br/> train_x_df, test_x_df, train_y_df, test_y_df = train_test_split(all_reviews_df[['reviews_content']], all_reviews_df[['category']])</span><span id="6367" class="jo jp hi jk b fi ju jr l js jt"> pl = Pipeline(steps=[('doc2vec', Doc2VecTransformer()),<br/> ('pca', PCA()),<br/> ('logistic', LogisticRegression())<br/> ])</span><span id="f469" class="jo jp hi jk b fi ju jr l js jt"> param_grid = {<br/> 'doc2vec__vector_size': [x for x in range(100, 250)],<br/> 'pca__n_components': [x for x in range(1, 50)]<br/> }<br/> gs_cv = GridSearchCV(estimator=pl, param_grid=param_grid, cv=5, n_jobs=-1,<br/> scoring="accuracy")<br/> gs_cv.fit(train_x_df[['reviews_content']], train_y_df[['category']])</span><span id="6f4c" class="jo jp hi jk b fi ju jr l js jt"> print("Best parameter (CV score=%0.3f):" % gs_cv.best_score_)<br/> print(gs_cv.best_params_)<br/> predictions_y = gs_cv.predict(test_x_df[['reviews_content']])<br/> print('Accuracy: ', metrics.accuracy_score(y_true=test_y_df[['category']], y_pred=predictions_y))</span></pre><p id="de50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但上面的一个将花费大量的时间来获得最佳设置。因此，简而言之，你可以减少可能的范围，仍然得到或多或少的最优设置。下面的代码片段就是为此准备的:</p><pre class="jf jg jh ji fd jj jk jl jm aw jn bi"><span id="b4e4" class="jo jp hi jk b fi jq jr l js jt">def train_short_range_grid_search():<br/> all_reviews_df = _read_all_reviews_()<br/> train_x_df, test_x_df, train_y_df, test_y_df = train_test_split(all_reviews_df[['reviews_content']], all_reviews_df[['category']])</span><span id="5fcb" class="jo jp hi jk b fi ju jr l js jt"> pl = Pipeline(steps=[('doc2vec', Doc2VecTransformer()),<br/> ('pca', PCA()),<br/> ('logistic', LogisticRegression())<br/> ])</span><span id="4cb8" class="jo jp hi jk b fi ju jr l js jt"> param_grid = {<br/> 'doc2vec__vector_size': [200, 220, 250],<br/> 'pca__n_components': [50, 75, 100]<br/> }<br/> gs_cv = GridSearchCV(estimator=pl, param_grid=param_grid, cv=3, n_jobs=-1,<br/> scoring="accuracy")<br/> gs_cv.fit(train_x_df[['reviews_content']], train_y_df[['category']])</span><span id="fe9d" class="jo jp hi jk b fi ju jr l js jt"> print("Best parameter (CV score=%0.3f):" % gs_cv.best_score_)<br/> print(gs_cv.best_params_)<br/> predictions_y = gs_cv.predict(test_x_df[['reviews_content']])<br/> print('Accuracy: ', metrics.accuracy_score(y_true=test_y_df[['category']], y_pred=predictions_y))</span></pre><p id="8c71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面我们得到的准确度分数是55%左右。这可以通过选择正确的模型或方法来改善。你可以在<a class="ae je" href="https://github.com/avisheknag17/public_ml_models" rel="noopener ugc nofollow" target="_blank"> github库</a>中获得完整的代码库。</p><p id="3a07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih jd">结论&amp;方法优化</strong></p><p id="9271" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在机器学习中，有一点是非常明显的“没有模型是绝对正确或错误的”。每种模式都有其利弊。上述问题也可以用不同的方式解决。事实上，如果我采取完整的深度学习方法，那么它可以给出更好的结果。但是我的意图是提供一个使用PCA和逻辑回归的技术的演示，这样你就可以把它应用于任何文本分析问题。</p><p id="174a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的方法是“如何在‘线性分类模型’中使用‘非线性特征’”。“Doc2Vec”肯定是使用神经网络从文档中提取的非线性特征，而逻辑回归是线性和参数分类模型。在机器学习中解决问题的四种组合是很常见的:“简单特征-简单模型”、“简单特征-复杂模型”、“复杂特征-简单模型”和“复杂特征-复杂模型”。我的方法属于“复杂特性-简单模型”类别。对数学概念和软件工程经验和技能的扎实掌握将赋予你选择正确方法的直觉/分析能力。</p><p id="c8da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不管怎样！！下次会提出另一个话题。在那之前，享受“机器学习”和“快乐编码”</p><p id="4223" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><p id="1f7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a) <a class="ae je" href="https://radimrehurek.com/gensim/tutorial.html" rel="noopener ugc nofollow" target="_blank"> Python-Gensim教程</a></p><p id="3867" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b) <a class="ae je" href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" rel="noopener ugc nofollow" target="_blank">带数学概念的PCA教程</a></p><p id="66c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c) <a class="ae je" href="http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/" rel="noopener ugc nofollow" target="_blank">带数学概念的逻辑回归教程</a></p></div><div class="ab cl kj kk gp kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hb hc hd he hf"><p id="8c60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最近，我写了一本关于ML(<a class="ae je" href="https://twitter.com/bpbonline/status/1256146448346988546" rel="noopener ugc nofollow" target="_blank">https://twitter.com/bpbonline/status/1256146448346988546</a>)的书</p><p id="5b80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kq">原载于2019年3月21日</em><a class="ae je" rel="noopener" href="/@avisheknag17/a-text-classification-approach-using-vector-space-modelling-pca-2a23fc2d66d7"><em class="kq">medium.com</em></a><em class="kq">。</em></p><figure class="jf jg jh ji fd ks er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es kr"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="jo jp hi bd kv kw kx ky kz la lb lc ld iq le lf lg iu lh li lj iy lk ll lm ln bi translated">这篇文章发表在<a class="ae je" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有+436，678名读者。</h2><h2 id="869c" class="jo jp hi bd kv kw kx ky kz la lb lc ld iq le lf lg iu lh li lj iy lk ll lm ln bi translated">在这里订阅接收<a class="ae je" href="https://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="jf jg jh ji fd ks er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es kr"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>