<html>
<head>
<title>Use PySpark for Your Next Big Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PySpark解决您的下一个大问题</h1>
<blockquote>原文：<a href="https://medium.com/swlh/use-pyspark-for-your-next-big-problem-8aa288d5ecfa?source=collection_archive---------10-----------------------#2019-06-15">https://medium.com/swlh/use-pyspark-for-your-next-big-problem-8aa288d5ecfa?source=collection_archive---------10-----------------------#2019-06-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/bda4ad7eaf7742703c4a637de0b4f350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z68F4SMRusBjJ7Q8ox4Waw.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@nasa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">NASA</a> on <a class="ae hv" href="https://unsplash.com/search/photos/big-data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="300e" class="pw-subtitle-paragraph iv hx hy bd b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm dx translated">利用大数据的力量</h2></div><p id="6f5f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">大数据正成为所有商业领域的热门词汇，因为它能够更完整地回答更多问题，并让人们对数据质量更有信心。问题是你如何利用这种力量解决你的下一个问题？</p><p id="2d74" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Spark是一个集群计算平台。群集是一组松散或紧密连接的计算机，它们一起工作来执行相同的任务。Spark允许您在具有多个节点的集群上传播信息和计算(将每个节点想象成一台不同的计算机)。分割数据使处理大型数据集变得更加容易，因为每个节点只处理少量数据。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kj"><img src="../Images/150e2f002d74f50bea2b10cd9f45c238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGMpCZJ6vVKhg773UeHwQQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Simple Cluster Computing Architecture</figcaption></figure><p id="9c93" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">每个节点处理自己的全部数据子集，还执行所需的部分全部计算，因此数据处理和计算都是在集群中的节点上并行执行的。并行计算可以使某些类型的编程任务更快。</p><p id="80f3" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然而，随着计算能力的增加，处理数据和创建有意义的解决方案的复杂性也增加了。决定Spark是否是您的问题的最佳解决方案并不总是一目了然的，但是可以问一些有用的问题:</p><ul class=""><li id="0eaa" class="ko kp hy jp b jq jr jt ju jw kq ka kr ke ks ki kt ku kv kw bi translated">我的数据是否太大，无法在单台机器上处理？</li><li id="ad25" class="ko kp hy jp b jq kx jt ky jw kz ka la ke lb ki kt ku kv kw bi translated">我的计算很容易并行吗？</li></ul><p id="8196" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">连接到集群是使用Spark的第一步。通常，集群将被托管在一个链接到所有其他节点的远程机器上。将会有一台被称为主机的计算机管理数据和计算的分割。主计算机连接到群集中的其余计算机，这些计算机称为从计算机。主机将数据发送给，并将计算结果发送给从机运行，然后从机将结果发送回主机。当您刚刚开始使用Spark时，在本地运行集群更简单——尽管原理是相同的。</p><p id="22b3" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Spark的核心数据结构是弹性分布式数据集(RDD)。这是一个底层对象，它允许Spark通过在集群中的多个节点上分割数据来发挥其魔力。然而，rdd很难直接使用，所以在本文中，我将使用构建在rdd之上的DataFrame抽象。</p><p id="7fa5" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Spark数据帧的设计行为很像SQL表(列中有变量，行中有观察值的表)。与rdd相比，Spark数据帧更易于理解和操作，并且在复杂操作中得到更好的优化。</p><p id="1685" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">当您开始修改和组合数据的列和行时，有许多方法可以达到相同的结果，但有些方法通常比其他方法花费的时间长得多。当使用rdd时，由数据科学家来找出优化查询的正确方法，但是使用Spark DataFrame实现的美妙之处在于它内置了很多这种优化。</p><p id="b487" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">您总是可以使用<code class="du lc ld le lf b">.toPandas()</code>将Spark数据帧转换成pandas数据帧，反之亦然,<code class="du lc ld le lf b">.createDataFrame()</code>方法获取pandas数据帧并返回Spark数据帧。然而，在PySpark中工作很容易，根本不用熊猫。</p><p id="c527" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在这篇文章中，我将使用UCI机器学习公司提供的默认信用卡客户数据集。该数据集包含从2005年4月到2005年9月台湾信用卡客户的违约付款、人口统计因素、信用数据、付款历史和账单等信息。</p><p id="4d15" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">首先，我们需要在我们的环境中建立一个SparkSession。</p><figure class="kk kl km kn fd hk"><div class="bz dy l di"><div class="lg lh l"/></div></figure><p id="c223" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在，我们将数据导入我们的环境。您可以使用<code class="du lc ld le lf b">.read</code>阅读csv文件和其他格式</p><figure class="kk kl km kn fd hk"><div class="bz dy l di"><div class="lg lh l"/></div></figure><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es li"><img src="../Images/410814bea62b52355c102e7e5316895f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDHuV0xPFzFaivIxtDSsKw.png"/></div></div></figure><p id="df04" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对SQL的基本理解将有助于管理Spark数据帧，因为操作是类似的。</p><pre class="kk kl km kn fd lj lf lk ll aw lm bi"><span id="adbe" class="ln lo hy lf b fi lp lq l lr ls"># Add column to DataFrame<br/>data = data.withColumn("colname",new_column)</span><span id="45f4" class="ln lo hy lf b fi lt lq l lr ls"># Filter data with a SQL string<br/>filtered_data = data.filter("colname &gt; value")</span><span id="8c86" class="ln lo hy lf b fi lt lq l lr ls"># Filter data with a boolean column<br/>filtered_data2 = data.filter(data.colname &gt; value)</span><span id="290a" class="ln lo hy lf b fi lt lq l lr ls"># Select the first set of columns<br/>selected1 = data.select("colname1", "colname2", "colname3")</span><span id="d14d" class="ln lo hy lf b fi lt lq l lr ls"># Group by column<br/>grouped_data = data.groupBy("colname")</span><span id="d724" class="ln lo hy lf b fi lt lq l lr ls"># Join the DataFrames<br/>joined_data = data1.join(data2, on=”common_colname”, how=”join_type”)</span></pre><p id="aef3" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在我们有了数据，下一步是构建模型以从数据中获得洞察力。这里首先要注意的是，PySpark需要数值数据来执行机器学习。为了将数据转换成数字形式，可以使用cast函数来创建所需数据类型的新列。</p><p id="a2cf" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对于本文，我不会做任何数据预处理，所以我将跳到管道中的最后一步——将包含我们的特性的所有列合并到一个列中。这必须在建模之前完成，因为每个Spark建模程序都希望数据是这种形式。可以通过将列中的每个值存储为vector中的一个条目来实现这一点。然后，从模型的角度来看，每一个观察都是一个向量，包含了关于它的所有信息和一个标签，告诉建模者这个观察对应于什么值。</p><p id="8a1b" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du lc ld le lf b">pyspark.ml.feature</code>子模块包含一个名为VectorAssembler的类。这个转换器接受您指定的所有列，并将它们组合成一个新的向量列。</p><p id="1757" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Pipeline是pyspark.ml模块中的一个类，它结合了您已经创建的所有估算器和转换器。通过将建模过程包装在一个简单的对象中，您可以一遍又一遍地重复使用相同的建模过程。</p><figure class="kk kl km kn fd hk"><div class="bz dy l di"><div class="lg lh l"/></div></figure><p id="c613" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">根据数据类型，管道中需要包含不同的方法。PySpark有处理内置在<code class="du lc ld le lf b">pyspark.ml.features</code>子模块中的字符串的函数。您可以创建“一次性向量”来表示无序的分类数据。独热向量是一种表示分类特征的方式，其中每个观察值都有一个向量，其中所有元素都为0，除了最多一个元素的值为1。向量中的每个元素都对应于功能的一个级别，因此可以通过查看向量中的哪个元素等于1来判断正确的级别。</p><p id="d2d5" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对分类特征进行编码的第一步是创建一个StringIndexer。该类的成员是估计器，它们采用带有一列字符串的数据帧，并将每个唯一的字符串映射到一个数字。然后，估计器返回一个转换器，该转换器接受一个数据帧，将映射作为元数据附加到该数据帧上，并返回一个新的数据帧，该数据帧带有一个对应于字符串列的数字列。</p><p id="7f0f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">第二步是使用OneHotEncoder将这个数字列编码为一个hot vector。这与StringIndexer的工作方式完全相同，先创建一个估计器，然后创建一个转换器。最终结果是一个列，它将您的分类特征编码为一个适合机器学习例程的向量！</p><pre class="kk kl km kn fd lj lf lk ll aw lm bi"><span id="e629" class="ln lo hy lf b fi lp lq l lr ls"># Create a StringIndexer<br/>string_indexer = StringIndexer(inputCol=”inputCol”, outputCol=”outputCol”)</span><span id="3324" class="ln lo hy lf b fi lt lq l lr ls"># Create a OneHotEncoder<br/>one_encoder = OneHotEncoder(inputCol=”inputCol”, outputCol=”outputCol”)</span></pre><p id="e960" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在使用将数据分成训练和测试，然后创建您选择的ml模型的实例。</p><figure class="kk kl km kn fd hk"><div class="bz dy l di"><div class="lg lh l"/></div></figure><p id="9b5b" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在我们拟合模型，然后在测试集上测试它。</p><figure class="kk kl km kn fd hk"><div class="bz dy l di"><div class="lg lh l"/></div></figure><p id="502d" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">就是这样，您已经使用PySpark对某人是否可能拖欠信用卡付款进行了分类，使用了一个框架，该框架允许您扩展该项目以使用更大的数据集，这可能会提供更准确的结果。</p><h2 id="9c55" class="ln lo hy bd lu lv lw lx ly lz ma mb mc jw md me mf ka mg mh mi ke mj mk ml mm bi translated"><strong class="ak">结论</strong></h2><p id="f3b2" class="pw-post-body-paragraph jn jo hy jp b jq mn iz js jt mo jc jv jw mp jy jz ka mq kc kd ke mr kg kh ki hb bi translated">PySpark是一个很好的工具，用于在大型数据集上执行机器学习，这些数据集太大，无法在单台机器上运行，并且计算可以很容易地并行化。工作流和代码与任何其他python库非常相似，并允许您利用大数据和集群计算的力量。</p><h2 id="81e9" class="ln lo hy bd lu lv lw lx ly lz ma mb mc jw md me mf ka mg mh mi ke mj mk ml mm bi translated"><strong class="ak">关键词汇</strong></h2><p id="30ff" class="pw-post-body-paragraph jn jo hy jp b jq mn iz js jt mo jc jv jw mp jy jz ka mq kc kd ke mr kg kh ki hb bi translated">pyspark——Apache Spark的Python实现</p><p id="6d07" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">群集—一组松散或紧密连接的计算机，它们一起工作以执行相同的任务</p><p id="2d8d" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">节点—用作执行任务的服务器的计算机</p><p id="0032" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">并行计算—一种计算类型，其中许多计算或流程的执行同时进行。</p><p id="d098" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">RDD —弹性分布式数据库</p></div></div>    
</body>
</html>