# 算法也有性别模式

> 原文：<https://medium.com/swlh/algorithms-have-gender-schemas-too-38d0dd244cd0>

## 这是一个关于在人工智能中我们没有积极阻止的事情和我们没有努力争取的事情的故事

![](img/be43714baae704c566b8f71e8b5a2a81.png)

阅读下面的[谜语](https://mikaelawapman.com/tag/riddle/)，如果你以前没听过，试着解答一下。

> **一位父亲和他的儿子开车外出，遭遇了一场可怕的车祸。父亲当场死亡，儿子生命垂危。儿子被紧急送往医院，准备进行一次可以挽救他生命的手术。外科医生进来，看到病人，大声说，“我不能做手术，那个男孩是我的儿子！”外科医生是谁？**

你做得怎么样？你考虑过外科医生是男孩母亲的可能性吗？如果你没有，你并不孤单。在一项[研究](http://www.bu.edu/today/2014/bu-research-riddle-reveals-the-depth-of-gender-bias/)中，197 名波士顿大学心理学学生中只有 14%和波士顿附近夏令营的 103 名儿童中只有 15%给出了“母亲是外科医生”的答案。

如果让计算机来解答这个谜语，它会怎么样？约翰霍普金斯大学的研究人员[通过使用](https://arxiv.org/pdf/1804.09301.pdf)[斯坦福 CoreNLP 基于规则的共指系统](https://stanfordnlp.github.io/CoreNLP/coref.html#overview)测试了一个类似的范式，该系统可以识别指代同一事物的单词或短语。回到我们的外科医生谜语，下面是研究人员用系统测试的三个句子，每个句子都有不同版本的外科医生:一个用男性代词，一个用中性代词，一个用女性代词:

> 外科医生不能给他的病人做手术:那是他的儿子！
> 
> 外科医生不能给他们的病人做手术:那是他们的儿子！
> 
> 外科医生不能给她的病人做手术:那是她的儿子！

当提供男性和中性代词句子时，系统正确地识别出，在第一个例子中，“his”指的是“外科医生”，在第二个例子中，“their”指的是“外科医生”。然而，在女性代词句子中，系统无法解决这个代词难题；相反，系统认为“它”是指代“外科医生”的代名词。

这只是算法性别偏见的一个例子。算法性别偏见代表了如何像我们一样，算法经常通过性别的镜头来检查世界，在性别模式的指导下操作。正如人类大脑利用性别模式将世界上遇到的刺激分类为“男性”或“女性”，算法也形成并利用自己的性别模式。

具有讽刺意味的是，这些算法性别模式代表了与创新对立的价值观。它们使偏见、成见和不平等永久化，同时拥有逆转这些进程的能力。

在考虑如何根除算法性别模式的想法之前，让我们探索一下它们目前在技术中具体化的三种方式。

## #1:来自技术创造者的算法性别模式

算法偏差[称](https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-the-birth-of-machine-bias)于 1979 年首次出现，当时圣乔治医院的医学院使用一个程序来自动化潜在学生的申请人筛选过程。从表面上看，该算法是成功的，当与招生选拔小组核对时，准确率达到了 90%到 95%。然而，这种准确性也导致了对妇女和非欧裔人的长期歧视。

技术上的一些偏差可能不是那么不经意的。根据联合国教科文组织 2019 年[关于数字技能和性别的报告](https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1)，女性仅占美国机器学习研究人员的 12%，占全球移动应用程序或软件开发者的 6%。这种不平衡可以部分解释高层决策中的性别偏见。例如，最近《纽约时报》的一篇[文章](https://www.nytimes.com/2019/05/22/world/siri-alexa-ai-gender-bias.html?te=1&nl=in-her-words&emc=edit_gn_20190524)引起了人们对表示性别的最权威方式之一——命名——如何也适用于虚拟助手的关注:亚马逊、苹果和微软分别将其广受欢迎的虚拟助手命名为 Alexa、Siri 和 Cortana。

无论性别是否是决策的一个重要方面，决策者的性别偏见肯定会在结果中被效仿。

## #2:由性别化的经济力量决定的算法性别模式

算法性别模式形成的下一个过程是一个经济学问题。

来自伦敦商学院和麻省理工学院的研究人员进行了一项实地测试，以了解脸书将如何展示一个有意保持性别中立的广告，在 191 个不同国家的男性和女性中推广 STEM 职业。实地测试的结果是男性比女性多看了 20%的广告。在谷歌 AdWords、Instagram 和 Twitter 上发布广告时，也出现了类似的结果。

通过后续测试，研究人员对结果得出了一个解释:“女性眼球比男性眼球贵。”

在研究人员描述为“挤出”的过程中，向女性做广告的成本较高，导致消费品广告赢得屏幕后进行的广告拍卖，拍卖决定向特定人显示最有利可图的广告。换句话说，消费品市场中的性别差异广告可能是导致就业市场中性别差异广告的一个因素。

## #3:(机器)学会了算法性别模式和完美风暴

如果你还记得的话，我从未透露过对共指系统未能将“外科医生”等同于“她”的解释。部分答案构成了算法性别图式形成的第三个过程:学习。研究人员发现，他们测试职业相关共指问题的系统的性能与实际就业统计数据以及职业在网上被称为男性或女性的比率相关。这些算法(大概)是在这些已经倾斜的数据集上训练的，引导它们重建存在于我们社交世界中的模式。此外，为这些数据集提供信息的潜在经济因素以及这些系统的创建者并没有对这种内在偏见做出任何纠正。

这三个过程的结合构成了算法性别图式形成的完美风暴。社会中已经存在的偏见和不平等使得每一个过程成为可能。然而，人们可以想象完全相反的情况——设计用来定义社会的技术流程。

## 坐下来，放松，让技术的社会建构继续下去——或者不继续

请注意下面照片中独特定位的横杆，照片中是一个骑着脚踏框架自行车的女人；这是一个很好的例子，说明了这个时代女性穿长裙的规范如何促成了一种支持这些价值观的技术。

[![](img/606400052697b4d51297ffe0474e5e5e.png)](https://commons.wikimedia.org/wiki/File:Woman_with_Bicycle_1890s.jpg)

Step-through frame bicycle, 1890s

这辆自行车隐喻了性别规范是如何影响算法设计的，以及这些算法的含义。最糟糕的是，与自行车上明显奇特的横杆相比，当涉及算法时，这个过程更难检测。

## 想象一个打破性别模式的世界

我们不能坐视算法，尽管它们拥有巨大的技术荣耀和尚未开发的社会变革潜力，却继续延续非数字世界中存在的偏见、刻板印象和不平等。

在这里，我转向桑德拉·贝姆的观点，她是一位开拓性的心理学家，在性别研究方面取得了革命性的进展。在她 1998 年出版的《一个非传统的家庭》一书中，贝姆描述了她为了在孩子成长过程中为他们创造一个不那么性别模式化的世界而采取的令人印象深刻且耗时的行动。她有时会在一本儿童读物中划掉某个女性的名字，用一个男性的名字来代替，以证明男孩穿裙子并不反常。另一个例子是，她多次开车经过一个特定的建筑工地，仅仅因为她知道那里有一名女性建筑工人。

对于非性别心理学家来说，这些策略是否过于雄心勃勃而无法每天实施？要是有什么东西能让这种策略自动化就好了。这就是核心矛盾所在。

我们可以对 Alexa 这样的虚拟助手进行编程，在每次使用时随机假设一个男性、女性或中性的声音，这样虚拟助手就不会根深蒂固地陷入女性性别模式。

我们可以让虚拟助手为我们的孩子读故事，随机选择角色的名字和代词，这样他们在故事中的角色就不会受到隐性偏见的影响。

我们可以编写虚拟助手或应用程序，客观地决定哪个孩子在某一天做什么家务，这样这些决定就不会受到孩子性别的影响。

技术、算法、人工智能——我们反复听到这些将把我们带入未来。事实是，我们通过创造机器把自己锁在了现在，这些机器就像它们的人类创造者一样有缺陷，把我们困在了自己偏见的循环中。除非我们决定不再允许我们当前世界的社会规范来构建技术，否则我们无法到达未来。这将是真正的创新。