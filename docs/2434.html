<html>
<head>
<title>Anomaly detection in Tweets: Clustering &amp; Proximity based approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推文中的异常检测:基于聚类和邻近度的方法</h1>
<blockquote>原文：<a href="https://medium.com/swlh/anomaly-detection-in-tweets-clustering-proximity-based-approach-58f8c22eed1e#2019-04-20">https://medium.com/swlh/anomaly-detection-in-tweets-clustering-proximity-based-approach-58f8c22eed1e#2019-04-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cc4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何使用聚类算法和邻近分析(LOF baed)来发现twitter文本推文中的离群值/异常值。两种方法的比较</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/63157e543ba701a85538f47670e8dacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dSLd7Lu3G9ppMgSH"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo by <a class="ae ju" href="https://unsplash.com/@anniespratt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Annie Spratt</a> on <a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0bd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">异常/孤立点检测是最大似然领域的一个热门话题。它属于“无监督学习”过程。与“监督学习”不同，这里我们没有任何关于数据模式的先验知识。“异常或异常值”是指与我们整个数据集中的其他数据点不太相似的数据点。在本文中，我将讨论两种从“推特”数据中发现异常推文的方法以及它们的比较。</p><p id="3c4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用Python，Sci-kit learn，来自“pypi.org”的“pyod”库，Gensim，NLTK来解决这个问题。</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="f2d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">获取数据和问题描述</strong></p><p id="b9c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个问题，我将使用UCI机器学习知识库中的数据Twitter中的<a class="ae ju" href="https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter" rel="noopener ugc nofollow" target="_blank">健康新闻数据集</a> <strong class="ih kc">”。</strong>该数据集包含来自CNN、纽约时报、CBC等各种新闻频道的不同样本推文。毫无疑问，这些是文本数据，而推文不过是一两行带有新闻源链接的文本。下面的python代码可以读取下载的数据(对于我的论文，我只使用了' nytimeshealth.txt '。还有来自其他新闻来源的“txt”文件)来自上述来源:</p><pre class="jf jg jh ji fd kd ke kf kg aw kh bi"><span id="a527" class="ki kj hi ke b fi kk kl l km kn">def _read_all_health_tweets():</span><span id="efd5" class="ki kj hi ke b fi ko kl l km kn">all_tweets = {}<br/> file = open(‘../../data/Health-Tweets/nytimeshealth.txt’, ‘r’)<br/> lines = file.readlines()<br/> for index, line in enumerate(lines):<br/> parts = line.split(sep=’|’, maxsplit=2)<br/> tweet = “”.join(parts[2:len(parts)])<br/> all_tweets[index] = tweet</span><span id="dfc0" class="ki kj hi ke b fi ko kl l km kn">file.close()<br/> return all_tweets</span></pre><p id="bf26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果上面的python“dict”被转换为pandas“data frame”并与“IPython-Jupiter Notebook”一起显示，那么前9个条目将如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kp"><img src="../Images/2696841c5fcc6864e01be546c0ecd149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EA9szCr2EeE8P_bU1UGYig.png"/></div></div></figure><p id="48db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该文件中共有6245条推文。很明显，并非所有的推文都与“健康新闻”相关，其中相当一部分可能属于任意领域。现在，我们的问题是:‘我们能识别出那些与医疗保健没多大关系的推文吗？’。或者我们能做的最好的事情，用最大似然术语来说就是“概括问题陈述”，也就是说，“我们能识别出前n个异常的推文吗？”其中“n”可以是用户提供的参数。这里的挑战是数据是完全“未分类/未标记的”(即，每条推文都没有“异常/非异常”的标签，我们无法用这些数据训练标准的ML模型)，从而使其在本质上“无监督”。现在我将提出两种方法来解决它，一种是“基于聚类”，另一种是“基于邻近”。但在此之前，文本tweet数据必须进行预处理，以便为任何类型的ML算法提供信息。我将在下一节讨论这个问题。</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="0b1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">预处理&amp;向量空间模型(Doc2Vec)生成</strong></p><p id="87de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预处理的第一步是将每个文本tweet转换成数组标记&amp;删除不想要的字符、停用词等。</p><p id="924f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步是将对应于每个文本tweet的每个标记数组转换成数字向量。这一步被称为‘向量空间模型’生成。有标准的向量空间模型可用，如“Tf-Idf”、“Word2Vec”、“Doc2Vec”等。该向量作为文档的特征向量工作，并确定有多少个特征。我使用“Doc2Vec”作为这个问题的模型，因为每个评论都可以被视为一个单独的文档，并且“Doc2Vec”在理解文本的上下文含义方面非常有效(与Tf-Idf相比，Tf-Idf只是一个纯粹的基于频率的模型)。</p><p id="1e30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下Python代码片段用作为文档创建Doc2Vec的管道步骤:</p><pre class="jf jg jh ji fd kd ke kf kg aw kh bi"><span id="bf8f" class="ki kj hi ke b fi kk kl l km kn">from sklearn.pipeline import Pipeline</span><span id="2c92" class="ki kj hi ke b fi ko kl l km kn">from gensim.models.doc2vec import TaggedDocument, Doc2Vec<br/>from sklearn.base import BaseEstimator<br/>from gensim.parsing.preprocessing import preprocess_string<br/>from sklearn import utils<br/>from tqdm import tqdm</span><span id="5ea1" class="ki kj hi ke b fi ko kl l km kn">class Doc2VecTransformer(BaseEstimator):</span><span id="a616" class="ki kj hi ke b fi ko kl l km kn">def __init__(self, vector_size=100, learning_rate=0.02, epochs=20):<br/> self.learning_rate = learning_rate<br/> self.epochs = epochs<br/> self._model = None<br/> self.vector_size = vector_size<br/> self.workers = multiprocessing.cpu_count() — 1</span><span id="8bc6" class="ki kj hi ke b fi ko kl l km kn">def fit(self, x, y=None):<br/> tagged_x = [TaggedDocument(preprocess_string(item), [index]) for index, item in enumerate(x)]<br/> model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)</span><span id="b5a3" class="ki kj hi ke b fi ko kl l km kn">for epoch in range(self.epochs):<br/> model.train(utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)<br/> model.alpha -= self.learning_rate<br/> model.min_alpha = model.alpha</span><span id="eba1" class="ki kj hi ke b fi ko kl l km kn">self._model = model<br/> return self</span><span id="1bb4" class="ki kj hi ke b fi ko kl l km kn">def transform(self, x):<br/> arr = np.array([self._model.infer_vector(preprocess_string(item))<br/> for index, item in enumerate(x)])<br/> return arr</span><span id="9bad" class="ki kj hi ke b fi ko kl l km kn">from sklearn.pipeline import Pipeline</span><span id="6b38" class="ki kj hi ke b fi ko kl l km kn">tweets_dict = _read_all_health_tweets()<br/>tweets = tweets_dict.values()<br/>pl = Pipeline(steps=[('doc2vec', Doc2VecTransformer())])<br/>vectors_df = pl.fit(tweets).transform(tweets)<br/>vectors_df</span></pre><p id="a96c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“vectors_df”将打印如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kq"><img src="../Images/8613fec9486d11675c3eaaf562bd4dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cMWz7paGI8IzgQ2y4c_pmQ.png"/></div></div></figure><p id="affc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面可以清楚地看到‘doc 2 vec’正在生成一个2D阵列。每行对应于每条文本推文。2D阵列的列是“Doc2Vec”的实际特征向量。这里，列数是100，由“vector_size”参数设置，这导致了特征数。该参数可以调整或由外部提供。因此，通过这种方法，每条文本推文都被转换成一组数字特征。这些特征是虚拟的，很难在物理上形象化。</p><p id="b110" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文档的“Doc2Vec”是通过对照随机采样的单词的邻近单词训练神经网络而生成的，反之亦然。这里，只有学习到的隐藏层的权重才是关注的问题，并被取为“Doc2Vec”值。事实上有两种技术:分布式内存(DM)和分布式单词包(DBOW)。有关更多详情，您可以查看以下资源:</p><ul class=""><li id="e51b" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated"><a class="ae ju" href="https://rare-technologies.com/doc2vec-tutorial/" rel="noopener ugc nofollow" target="_blank">稀有科技的Doc2Vec教程</a></li><li id="46cf" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated"><a class="ae ju" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">分布式陈述语句&amp;文档</a></li></ul></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="03d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">维度分析和主成分</strong></p><p id="f698" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的“Doc2Vec”转换为每条文本推文提供了100个特征向量。但是像聚类和离群因素这样的分析技术，太多的特征可能会破坏实际的目的。建议将“Doc2Vec”向量大小保持在100到300之间。因此，最好在“Doc2Vec”上做一个更高维的分量投影，看看哪些是重要的分量，哪些不是。“PCA”非常适合这一点。我分析了10个主要成分，发现只有前2个能够解释足够的差异。</p><p id="378b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，可能会出现一个问题，“为什么我没有将‘doc 2 vec’中的‘vector _ size’保持为2或3，而不是100。本来可以避免不必要的复杂的常设仲裁院。”回答是“没有”。如果我保持“vector_size”为3，那么在“Doc2Vec”中许多隐藏的特征将被抑制。也可能出现过早收敛问题。这就是为什么最好让‘doc 2 vec’生成所有确定大小的向量，然后应用PCA。</p><pre class="jf jg jh ji fd kd ke kf kg aw kh bi"><span id="4206" class="ki kj hi ke b fi kk kl l km kn">def analyze_tweets_pca(n_pca_components):<br/>    tweets_dict = _read_all_health_tweets()<br/>    tweets = tweets_dict.values()<br/>    doc2vectors = Pipeline(steps=[('doc2vec', Doc2VecTransformer())]).fit(tweets).transform(tweets)<br/>    pca = PCA(n_components=n_pca_components)<br/>    pca_vectors = pca.fit_transform(doc2vectors)<br/>    print('All Principal Components ..')<br/>    print(pca_vectors)<br/>    for index, var in enumerate(pca.explained_variance_ratio_):<br/>        print("Explained Variance ratio by Principal Component ",<br/> (index+1), " : ", var)</span><span id="1043" class="ki kj hi ke b fi ko kl l km kn">analyze_tweets_pca(n_pca_components=10)</span></pre><p id="da83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它产生以下输出:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lf"><img src="../Images/cfcd2edf8cbcc108443c637990b4bbac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ni6GunSIePqQc9RcM84CXg.png"/></div></div></figure><p id="954e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在接下来的章节中，我将只采用前2个主成分(几乎99.7%的方差由前2个主成分解释)进行聚类和局部异常因子估计。这样，维数减少了，并且还可以提高聚类和邻近分析的准确性。</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="fa03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">基于聚类的(k-means)方法</strong></p><p id="4f74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类是“无监督学习”中最流行的技术之一。我使用了标准的“k均值”算法。对于使用点的“轮廓分数”的“k均值”概念的异常值检测是非常重要的。</p><p id="e52c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> Silhoutte评分:</em>由类间和类内距离的比值给出，如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lg"><img src="../Images/6478f2334f2b78ea2e883fa51467f011.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/1*OZW_v29SVuw9SqnDd04gOg.gif"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 4</figcaption></figure><p id="2da7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里，</p><p id="2966" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b(i) =数据点“I”到任何其他聚类(其中“I”不是成员)中所有点的最小平均距离</p><p id="aac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a(i) =数据点“I”之间的平均距离。和同一聚类中的所有其他数据点</p><p id="1da4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的所有定义中，距离是作为PCA向量之间的余弦相似性的倒数来计算的，因为它对大小不太敏感&amp;对于文本分析来说，它优于“欧几里德”或广义“闵可夫斯基”距离</p><p id="3170" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有数据点的“轮廓得分”平均值是特定聚类设置的总体“轮廓得分”。它的值范围从-1到1。正值越大，表示集群设置越好。现在，从各个数据点的“轮廓分数”中，可以得出如下三个结论:</p><p id="f627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">值接近-1 : </em>数据点被错误地放入一个理想情况下不应该属于的簇中。基本上它是一个“内场”。</p><p id="ed8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">接近0的值:</em>数据点不应该属于任何聚类，应该分离出来。这是一个“离群值”或“异常值”。</p><p id="2094" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">接近1的值:</em>数据点被完美地放置在正确的聚类中。</p><p id="142a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上述第二点可以清楚地看出，从聚类设置中找出异常值无非是找出“轮廓得分”值最接近于零的数据点。按升序排列所有绝对“轮廓得分”值将给出可能的前n个异常值。可以看出，这些点通常位于聚类的边界区域之间。因为，从概念上讲，那些不属于任何集群。</p><p id="b81f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要做到这一点，首先应该使用“k-means”算法的最佳“k”值进行聚类。最佳“k”是整个聚类设置“轮廓分数”变得最大或接近+1的那个。这可以通过尝试一组“k”值范围来确定。我用两个方差最大的“主成分”做了“k均值”。</p><pre class="jf jg jh ji fd kd ke kf kg aw kh bi"><span id="9e13" class="ki kj hi ke b fi kk kl l km kn">def determine_anomaly_tweets_k_means(top_n):<br/>    tweets_dict = _read_all_health_tweets()<br/>    tweets = tweets_dict.values()<br/>    pl = Pipeline(steps=[('doc2vec', Doc2VecTransformer()),<br/>                         ('pca', PCA(n_components=2)),<br/>                         ('kmeans', OptimalKMeansTextsClusterTransformer(min_k=2, max_k=5))])<br/>    pl.fit(tweets)<br/>    pca_vectors, cluster_labels = pl.transform(tweets)<br/>    silhouette_values = silhouette_samples(X=pca_vectors, labels=cluster_labels, metric='cosine')<br/>    tweet_index_silhouette_scores = []<br/>    absolute_silhouette_scores_tweet_index = []<br/><br/>    for index, sh_score in enumerate(silhouette_values):<br/>        absolute_silhouette_scores_tweet_index.append((abs(sh_score), index))<br/>        tweet_index_silhouette_scores.append((index, sh_score))<br/><br/>    sorted_scores = sorted(absolute_silhouette_scores_tweet_index, key=sort_key)<br/><br/>    top_n_silhouette_scores = []<br/>    pca_vectors_anomalies = []<br/>    print("Top ", top_n, " anomalies")<br/>    for i in range(top_n):<br/>        abs_sh_score, index = sorted_scores[i]<br/>        index_1, sh_score = tweet_index_silhouette_scores[index]<br/>        top_n_silhouette_scores.append((index, sh_score))<br/>        print(tweets_dict[index])<br/>        print('PCA vector', pca_vectors[index])<br/>        pca_vectors_anomalies.append(pca_vectors[index])<br/>        print('Silhouette Score: ', sh_score)<br/>        print("..................")<br/><br/>    plot_tweets_k_means_clusters_with_anomalies(pca_vectors=pca_vectors, pca_vectors_anomalies=pca_vectors_anomalies,<br/>                                                cluster_labels=cluster_labels)<br/>    plot_scatter_silhouette_scores(top_n_silhouette_scores=top_n_silhouette_scores,<br/>                                   tweets_dict=tweets_dict,<br/>                                   silhouette_score_per_tweet=tweet_index_silhouette_scores)</span><span id="6a8e" class="ki kj hi ke b fi ko kl l km kn">determine_anomaly_tweets_k_means(top_n=5)</span></pre><p id="9dbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它产生以下输出和图形:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lh"><img src="../Images/29dcedeafe71311829027948469dc53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMWvvpaLx33addj1xDXlgQ.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es li"><img src="../Images/c764165a54ccd529f5c063d787b29055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uike6lKyCSN5Mmb2IEGQHw.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 1</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lj"><img src="../Images/e019b96ce451968bdd176ed70a5d6a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qlvNw4RSXGp6bC-WuMAhwQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 2</figcaption></figure><p id="cffa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图1可以看出，离群值(蓝色的“三角形”点)位于集群的边界区域。图2是‘轮廓散点图’，异常值显示为‘红色’点，证明异常值的‘轮廓分数’接近于零。</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="2c29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">基于局部离群因子(接近度)的方法</strong></p><p id="1357" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在详细讨论之前，需要给出一些指标的定义，如下所示:</p><ul class=""><li id="adf7" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">Nk(A):数据点A的k个最近邻居的集合</li><li id="9776" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">k-距离(A):第k个与其邻居最近的距离</li><li id="29e9" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">可达距离(A，B):数据点A &amp; B之间可达距离由该公式给出。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lk"><img src="../Images/f95f1139163cc5ccf0c1641cef5b7be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/1*EM48IeYALbF079iGO08Ttw.gif"/></div></figure><ul class=""><li id="7abc" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">局部可达密度(A)或lrd(A):数据点A与其邻居的平均可达距离的倒数，由以下公式给出:</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ll"><img src="../Images/489bf7c4b6182a3b1cc401a6402a142f.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/1*zLkXjTdIZqmpCB8rQyJVLA.gif"/></div></figure><ul class=""><li id="27b9" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">局部异常值因子(A)或lof(A):邻居的平均局部可达密度除以数据点A自己的局部可达密度，它由以下公式给出:</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/f3f7e21c059f6b84b2d3d6b3a56a99de.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/1*xIUYjaggsbHqqQ965uHe0A.gif"/></div></figure><p id="8a19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上述定义中，所有“距离”都是由余弦相似性的倒数给出的，类似于“k-means”方法。</p><p id="b550" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过lrd(A ),可以从A获得到达邻居或任何数据点聚类的平均行进距离。lrd(A)的值越低，其可达性将降低，即，它指示数据点距离其他数据点很远。</p><p id="9ac7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过lof(A ),可以比较点A与其相邻点的“lrd”。如果邻居的“lrd”较高，则该特定数据点远离密集区域，因此,“异常值”及其“异常值分数”由“lof”值给出。通常lof &gt; 1意味着“可能的异常值”。</p><p id="04f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在计算了所有数据点的“LOF”值后，按照降序对这些值进行排序将得到前n个可能的异常值，其中“n”作为输入参数。我对预处理过的“Twitter”数据应用了同样的方法，用两个主成分给出了最高的方差，得到了最高的离群值。</p><pre class="jf jg jh ji fd kd ke kf kg aw kh bi"><span id="81b1" class="ki kj hi ke b fi kk kl l km kn">from matplotlib import pyplot as plt<br/>from pyod.models.lof import LOF</span><span id="0cee" class="ki kj hi ke b fi ko kl l km kn">class LOFDetectionTransformer(BaseEstimator):<br/><br/>    def __init__(self):<br/>        self._model = None<br/><br/>    def fit(self, x, y=None):<br/>        self._model = LOF(metric='cosine')<br/>        self._model.fit(x)<br/>        return self<br/><br/>    def transform(self, x):<br/>        return self._model.decision_scores_</span><span id="2e66" class="ki kj hi ke b fi ko kl l km kn">def determine_anomaly_tweets_lof(top_n):<br/>    tweets_dict = _read_all_health_tweets()<br/>    tweets = tweets_dict.values()<br/>    pl = Pipeline(steps=[('doc2vec', Doc2VecTransformer()),<br/>                         ('pca', PCA(n_components=2)),<br/>                         ('lof', LOFDetectionTransformer())])<br/>    pl.fit(tweets)<br/>    scores = pl.transform(tweets)<br/>    tweet_index_decision_scores = []<br/>    decision_scores_tweet_index = []<br/><br/>    for index, score in enumerate(scores):<br/>        decision_scores_tweet_index.append((score, index))<br/>        tweet_index_decision_scores.append((index, score))<br/><br/>    sorted_scores = sorted(decision_scores_tweet_index, key=sort_key, reverse=True)<br/><br/>    top_n_tweet_index_decision_scores = []<br/>    print("Top ", top_n, " anomalies")<br/>    for i in range(top_n):<br/>        score, index = sorted_scores[i]<br/>        top_n_tweet_index_decision_scores.append((index, score))<br/>        print(tweets_dict[index])<br/>        print('Decision Score: ', score)<br/>        print("..................")<br/><br/>    plot_scatter_lof(tweets_dict=tweets_dict, tweet_index_decision_scores=tweet_index_decision_scores,<br/>                     top_n_tweet_index_decision_scores=top_n_tweet_index_decision_scores)</span><span id="b047" class="ki kj hi ke b fi ko kl l km kn">determine_anomaly_tweets_lof(top_n=5)</span></pre><p id="bf36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">调用上面的函数可以显示具有较高“决策得分”的前五个异常文本tweets(参数为“top_n ”),并生成以下输出和图表</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kp"><img src="../Images/a13ec5da99ead4e30c3a3e434950ad52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2E30K9KB0jHA4m8oHz78Q.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/e0c4f8aa48efd92e332133bc4973ebe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hJTlGcQrrU-O2ufQaXEgg.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 3</figcaption></figure><p id="7f70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的“决策分数”只不过是数据点的“lof”值。上面散点图中的所有异常值都显示为“红色圆圈”点。LOF算法取决于下面给出的两个重要参数</p><p id="9d1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> n个邻居:</em>计算“k-距离”和“lrd”时要考虑的数据点周围的邻居数量。通常它保持为20，但是通常它的值来自领域知识</p><p id="070c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">污染:</em>异常值与总人口数量的比率。通常它保持为0.1，但是和上面一样，它的值也来自领域知识。“污染”在确定群体的“阈值lof”中起着非常重要的作用。“阈值lof”可用于决定一个新数据点是否是异常值(在我们的例子中，一个新的文本tweet不存在于训练集中)</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="5310" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">两种方法的比较</strong></p><p id="c9fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“k-均值聚类”和“LOF”方法都有一些优点和缺点。在这两种情况下，可以在多次运行中看到输出的随机性。这主要是由于“Doc2Vec”的产生和那里的洗牌活动。</p><p id="72e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">k均值聚类方法的优势:</em></p><p id="807a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I)不需要数据的先验知识或领域知识。参数“k”可以动态确定。适用于没有数据知识的用例。</p><p id="ac8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">k均值聚类方法的缺点:</em></p><p id="3141" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I)运行时间更多。找出最佳“k”本身是一项相当耗时的活动</p><p id="06a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ii)如果簇的形状不是球形，则不能很好地工作。事实上，在这种情况下，“剪影分数”本身将开始给出错误的结果</p><p id="603c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iii)在看不见数据的情况下，很难确定异常值。</p><p id="7e32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">“LOF”方法的优势:</em></p><p id="43f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I)在任意形状的数据的情况下工作得非常好。它非常类似于“DBSCAN ”,并基于相同的“局部邻近”概念工作</p><p id="1d43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ii)运行时间更少。</p><p id="a937" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iii)对于看不见的数据，它也工作得很好。它可以在训练集中不存在的新数据上标记出“异常值/非异常值”。</p><p id="829d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">“LOF”方法的缺点:</em></p><p id="556b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I)参数需要领域知识:“污染”和“n个邻居”。</p><p id="7494" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，很明显，这两种方法实际上是相辅相成的。在做出任何选择特定方法的决定之前，必须有人彻底理解业务用例。</p><p id="1957" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案的代码库可以在<a class="ae ju" href="https://github.com/avisheknag17/public_ml_models/blob/master/health_tweets_anomaly_analysis.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="4a96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的Linkedin页面:<a class="ae ju" href="https://www.linkedin.com/in/avishek-nag-957a0015/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/avishek-nag-957a0015/</a></p><p id="ce2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最近，我写了一本关于ML(<a class="ae ju" href="https://twitter.com/bpbonline/status/1256146448346988546" rel="noopener ugc nofollow" target="_blank">https://twitter.com/bpbonline/status/1256146448346988546</a>)的书</p><p id="e7bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih kc">参考文献:</strong></p><ol class=""><li id="0b00" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc lo kx ky kz bi translated">本地异常因素—<a class="ae ju" href="http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf" rel="noopener ugc nofollow" target="_blank">http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf</a></li><li id="0503" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc lo kx ky kz bi translated">k-means算法—<a class="ae ju" href="https://www.datascience.com/blog/k-means-clustering" rel="noopener ugc nofollow" target="_blank">https://www.datascience.com/blog/k-means-clustering</a></li><li id="f87d" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc lo kx ky kz bi translated">主成分分析—<a class="ae ju" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" rel="noopener" target="_blank">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a></li></ol><figure class="jf jg jh ji fd jj er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es lp"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="ki kj hi bd lq lr ls lt lu lv lw lx ly iq lz ma mb iu mc md me iy mf mg mh mi bi translated">这篇文章发表在<a class="ae ju" href="https://medium.com/swlh" rel="noopener">《创业</a>》上，这是Medium最大的创业刊物，有+444678人关注。</h2><h2 id="869c" class="ki kj hi bd lq lr ls lt lu lv lw lx ly iq lz ma mb iu mc md me iy mf mg mh mi bi translated">在这里订阅接收<a class="ae ju" href="https://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es lp"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>