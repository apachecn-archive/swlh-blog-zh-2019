<html>
<head>
<title>Understanding The Math Behind Dimension Reduction in Facial Recognition(1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解面部识别降维背后的数学原理(1)</h1>
<blockquote>原文：<a href="https://medium.com/swlh/understanding-the-math-behind-dimension-reduction-in-facial-recognition-1-cb18368d23c0#2019-06-05">https://medium.com/swlh/understanding-the-math-behind-dimension-reduction-in-facial-recognition-1-cb18368d23c0#2019-06-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1fa2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">阅读我的初学者友好的证明，探索线性代数的应用</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/87f9f1b18c8a144161f09163bf25801e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOLFDcV3_XqeFEcEg7u9CA.jpeg"/></div></div></figure><p id="877a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">降维是计算机科学中的一种常见技术，你们大多数人可能都以某种方式遇到过，但并不是每个人都理解它是如何从数学上推导出来的。在这一系列文章中，我将通过一个简单的面部识别示例向您介绍降维背后的线性代数。除了理解数学，你还将看到降维如何有助于简化图像识别的任务。特别是，我将谈论特征向量/特征值，频谱定理和主成分分析。</p><h2 id="9f49" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">为什么需要降维</h2><p id="7639" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">为了重新唤起你对降维的记忆，让我们从一个假设的例子开始。米德尔伯里学院的通信部门存储了从每个米德尔伯里学院学生那里拍摄的3000张照片。系主任最近在当地报纸上发现了一张学生的照片，她想知道这个学生是否属于米德尔伯里。</p><p id="800b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于图像的多维性质，设计一个有效的面部识别模型是很困难的。如果把我们讨论的范围限定在黑白图像上，我们可以把一幅m × n像素的图像想象成一个矩阵，矩阵中填充了0(黑)到255(白)之间的实数。每个矩阵的维数为m × n，每个像素代表一个维数。</p><p id="19bd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果我们假设每张学生照片的尺寸为1，600 × 1，200，然后我们将新图像与3，000张照片中的每张照片逐个像素地进行比较，我们将需要进行3，000 × 1，600 × 1，200 = 5，760，000，000次比较。我们的目标是减少学生照片的尺寸，而不丢失(或最小化丢失)照片中捕捉到的差异。在深入研究具体步骤之前，我想先介绍一下方差的基本概念。</p><h2 id="8106" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">方差的直观概述</h2><p id="2e06" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">方差是平均值的平方差的平均值。例如，给定一列实数<code class="du lf lg lh li b">x_1...x_n</code>，我们将方差定义为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lj"><img src="../Images/8fa901e92d186f2c34b057f731f726d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*bPIkqPeEEqi6WUMbrxMZjg.png"/></div></figure><p id="b08c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在这个应用中，差异是区分一个人的塌鼻子和另一个人的鹰钩鼻的标准。我们希望米德尔伯里学生的3000张照片之间的差异尽可能高，因为高差异会提高识别的准确性。直观上，方差随着维数的增加而增加。如果给你一张完整的照片，而不仅仅是鼻子周围的像素，你会更容易识别你朋友的照片。然而一些维度可能是重复的。例如，彼此相邻的像素可能具有非常相似的值，因为它们捕捉了面部的相同种类的变化。</p><p id="7001" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">回想一下，在我们的例子中，导演从1600×1200 = 1920000维开始。在本文的剩余部分，我将演示基的改变如何帮助我们将维数缩减到t &lt; 1，920，00，而不损失任何方差。在下一篇文章中，我将演示主成分分析如何进一步将维数减少到m ≪ t，同时保留大部分方差。</p><h2 id="1ba3" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">协方差矩阵的构造</h2><p id="8d17" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">让我们从构建一个包含所有学生图像的真实向量空间开始。我们通过逐行连接像素，将学生图像的每个矩阵转换为行向量。例如，如果<code class="du lf lg lh li b">K₁</code>表示学生1的图像，则:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lk"><img src="../Images/72bd76598752052d363eec98481e8227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7wvTW6tlyrtrscRHpXxUA.png"/></div></div></figure><p id="56c8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，我们将行向量<code class="du lf lg lh li b">k₁ … k₃₀₀₀</code>组合在一起，形成一个3000×1920000的矩阵<code class="du lf lg lh li b">K_all</code>。<code class="du lf lg lh li b">K_all</code>的每一列代表一个维度(对应一个像素)，每一行代表一个面向量<code class="du lf lg lh li b">kᵢ</code>。所有<code class="du lf lg lh li b">K_all</code>的行向量一起形成了我们的面部空间ℝ ⁹ ⁰⁰⁰⁰:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ll"><img src="../Images/a881e16815e8dc7efe80a668b9498148.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*VUmSZ2pS47f5WiEpo553jQ.png"/></div></figure><p id="743f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了强调图像之间的差异并简化未来的计算，我们首先计算平均人脸向量<code class="du lf lg lh li b">k̄</code>，即所有3000张人脸图像中每个像素的平均值。然后，我们通过从这个矢量中减去平均人脸矢量<code class="du lf lg lh li b">k̄</code>来归一化每个人脸矢量<code class="du lf lg lh li b">kᵢ</code>。设<code class="du lf lg lh li b">vᵢ</code>为归一化后的iᵗʰ人脸差向量，则:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/063573669c7201ef003700a191843690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*jywCH6RtvP1tCeSaOQfZdA.png"/></div></figure><p id="8be6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">类似地，我们组合面部差异向量<code class="du lf lg lh li b">v₁ … v₃₀₀₀</code>以形成方差矩阵A。A的每一列代表一个维度，每一行代表一个面部差异向量:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/519ba4ae3f5ecd7a7de411e1a2c96bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*FWfMSW66Z9NAUewKlM0qFA.png"/></div></figure><p id="b71b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了用最少的维度最大化方差，我们希望每个维度都是独立的，即在维度x中捕获的方差不应与在维度y中捕获的方差重叠。在数学中，协方差是衡量两个维度的联合可变性的概念。协方差公式表明:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/9eae8885cc321a8b54529d27e06c4a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*B6c_xW79DgDJYGCa7C5juQ.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 1</figcaption></figure><p id="9962" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在我们的例子中，由于我们已经归一化了我们的面部向量<code class="du lf lg lh li b">x̄ = ȳ = 0</code>，我们可以通过方差矩阵a的x和y列的点积直接计算维度x和y之间的协方差。让<code class="du lf lg lh li b">d_x</code>和<code class="du lf lg lh li b">d_y</code>表示a的xᵗʰ和yᵗʰ列(对应于像素x和y)。还设α是⅟3000，是a中行数的倒数，则:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/3a0eb33b061397967856581c0207f2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*ZDqRpKIHMYaCvAX-K8nH_g.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 2</figcaption></figure><p id="4035" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了展示矩阵a中每两维之间的协方差，我们构造协方差矩阵c。让<code class="du lf lg lh li b">dᵢ</code>表示矩阵a的iᵗʰ列:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/daee0a0940114430c8be1a71ff81daa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*mSiEwZ63B2H8-pscb3dNhQ.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 3</figcaption></figure><p id="43d2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">结合等式2和3，我们得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/b325665e3ea6bfbdf552abdb1b7ba4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*gOM83c3OPWXkHUz86YHTfw.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 4</figcaption></figure><p id="b89f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">协方差矩阵C是对称的，我们可以从等式2中推断出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/dcf5b9c1518c2c24687d002a66ee24cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*Bw77iEixpZcImqNS6VXAVQ.png"/></div></figure><p id="1453" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">C的对角线表示每个维度上的方差。</p><h2 id="7e7b" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">改变正交基:谱定理</h2><p id="39cf" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">一旦我们构建了协方差矩阵C，我们希望找到一个新的正交基，对于新基中表示的任何x和y以及x ≠ y，Cov(x，y) = 0。这个新基是最佳的，因为每个维度上方差的效用将最大化，因为跨维度的方差没有重叠。由于我们已经展示的谱定理和C的对称性，我们可以对C进行基变换:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/c2e2dcc952d4eb3cafe339397f6905fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*VKe_WxcltVS0gwyelGy6Aw.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 5</figcaption></figure><p id="06db" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">其中D是实对角矩阵，Q是正交矩阵。C的特征值出现在d的对角线上，Q，<code class="du lf lg lh li b">q₁, q₂, …, q₁₉₂₀₀₀₀</code>的列是对应的标准正交特征向量，它们组成了我们想要的新的标准正交基。</p><p id="7f40" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">通过基的变换，<code class="du lf lg lh li b">q₁, q₂, …, q₁₉₂₀₀₀₀</code>现在形成了我们人脸空间的一个新的正交基，这个基中任意两维之间的协方差为0。根据谱定理，这个正交基恰好是本征基。我们可以验证这个特征基是正交的，因为D是对角矩阵，这使得Cov(x，y) = 0，对于基中表示的任何x和y，x ≠ y。</p><p id="b700" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">到目前为止，我们已经消除了维度之间的协方差。然而，细心的读者可能会注意到，我们仍然有相同数量的维度，因为新的正交基由1，920，000个特征向量组成，每个特征向量跨越一个维度。如果是这样，构造正交本征基如何帮助我们降维？事实证明，如果这些特征向量对应于零特征值，我们可以安全地从正交特征基中移除所有特征向量，因为它们对总方差没有贡献。为了验证这一说法，我将继续证明每个特征向量所跨越的维度上的方差等于与该特征向量相关联的特征值(<em class="ly">引理1 </em>)，因此零特征值表示零方差。如果不想看详细的证明，可以跳到<a class="ae lz" href="#2668" rel="noopener ugc nofollow">下一节</a>。</p><blockquote class="ma mb mc"><p id="7910" class="jj jk ly jl b jm jn ij jo jp jq im jr md jt ju jv me jx jy jz mf kb kc kd ke hb bi translated"><em class="hi">引理1:设u是矩阵C= α AᵗA的正交本征基的一个本征向量，其中α是a中行数的倒数.设λ是u对应的本征值.设</em> <code class="du lf lg lh li b">v_1...v_n</code> <em class="hi">是a的归一化行向量.设σᵤ是</em> <code class="du lf lg lh li b">v_1...v_n</code> <em class="hi">在u跨越的维数上的方差.我们称σᵤ = λ.</em></p></blockquote><p id="eddc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了证明<em class="ly">引理1 </em>，根据正交投影公式，在欧几里得内积下，设pᵢ是vᵢ在u所跨越的维度上的投影权重:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/1bf2c1fde961c937a09136afcb9c5086.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*Fw6yjSkdUcyzyOIZ9m9CuA.png"/></div></figure><p id="3bb1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">设p为包含<code class="du lf lg lh li b">p_1...p_n</code>的投影测量向量:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/04a4cae02c770022dd1bab89398b7e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*T5AYNLW7al6HfuNntixFAA.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 6</figcaption></figure><p id="d8c4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于<code class="du lf lg lh li b">v_1...v_n</code>已经正常化，<code class="du lf lg lh li b">v_1 + ... + v_n = 0</code>。因此:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/7ad8ef4f97fbff3302bf77def83acd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*GCLkLSR0RKqqAzNQXhdnNg.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 7</figcaption></figure><p id="22d6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">根据方差公式:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/4040613e7dfc9befe2745d438d1d175f.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*CVgqP2140v_CYaptEzGoXw.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 8</figcaption></figure><p id="7b81" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">结合等式6和等式8，我们得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/31c1fb4aac563a47626dfdc8b5f30676.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*ur_TJiHf8hb_WbJm2vXdng.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 9</figcaption></figure><p id="e5df" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">回想一下，对于任意两个相同长度矢量v和w:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ml"><img src="../Images/40b485864920a5f99a73dc0c059ac45a.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*CYhKqQHiIXmOxNUfZdPW0Q.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 10</figcaption></figure><p id="3a29" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">结合等式9和10，我们得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mm"><img src="../Images/0a12bb2bf706775581ae65e620324c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*LlHo7bwBHdI1MHwjkmcO_w.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">equation 11</figcaption></figure><p id="7602" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">结合等式9和11，我们得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/dcce2acf05b126a7a403b39b9bae5371.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/format:webp/1*iqEH4LiAkRbdosaoVandLw.png"/></div></figure><p id="8eac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在证明了<em class="ly">引理1 </em>之后，我们现在确认移除具有零特征值的特征向量不会影响方差。此外，在面部空间中捕获的总方差是c的所有特征值的和。</p><h2 id="2668" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">AᵗA和AAᵗ之间的转换</h2><p id="a184" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">到目前为止，我们已经证明了具有零特征值的特征向量对方差没有贡献。如果我们从C的特征基中移除具有零特征值的T个特征向量，由剩余特征向量形成的子空间T将仅具有维度<code class="du lf lg lh li b">1,920,000 — t</code>，但是T将捕获与我们的原始面部空间相同量的方差，其具有1，920，000的维度。问题是:我们如何最大化t，从而减少尽可能多的维度？</p><p id="b385" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让Cᵗ = αAAᵗ.我们注意到Cᵗ只有3000 × 3000的维数，明显小于C = αAᵗA.的维数，因此我们继续研究Cᵗ和c之间的关系。根据谱定理，Cᵗ也具有正交本征基，因为很明显Cᵗ是对称的:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mo"><img src="../Images/8632964084493d8b607d789ede1b06b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*TL-LukXH8Gh1PNvIGD1lMw.png"/></div></figure><p id="27b6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果我们能证明c中每一个具有非零特征值的特征向量在Cᵗ中有一个对应的具有相同特征值的特征向量，我们就能证明c中的特征值之和与小得多的Cᵗ.中的特征值之和相同根据<a class="ae lz" href="#7910" rel="noopener ugc nofollow">引理1 </a>，我们可以证明Cᵗ的本征基与c的本征基具有相同的方差。因此，我们可以将t最大化到1，920，000–3，000 = 1，917，000。因此，我们可以形成3，000维的子空间T，它携带与我们原始的1，920，000维的面部空间相同的方差。</p><p id="c269" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们继续证明上述主张，即c中具有非零特征值的每个特征向量在Cᵗ中具有相同特征值的对应特征向量:</p><blockquote class="ma mb mc"><p id="8d80" class="jj jk ly jl b jm jn ij jo jp jq im jr md jt ju jv me jx jy jz mf kb kc kd ke hb bi translated">引理2:若q是α AᵗA (α ≠ 0)的一个特征值λ非零的特征向量，则Aq是α AAᵗ的一个特征值λ相同的特征向量。</p></blockquote><p id="3114" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了证明<em class="ly">引理2 </em>，设q为α AᵗA的一个特征向量，对应一个非零特征值λ:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mp"><img src="../Images/4f6d7bc8baad72caf654c727af369a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*FKGdF56KwcLz8a5MWP2V5Q.png"/></div></figure><p id="ac75" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们还需要证明Aq ≠ 0。如果Aq = 0，则:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/738de7ecd92afae2ca5042bdeef0e748.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*0w5sjxfQPUD8XhtjFH1ynw.png"/></div></figure><p id="aea9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然而，q是具有非零特征值λ的特征向量的事实意味着q ≠ 0且λ ≠ 0。因此出现了矛盾。所以，Aq≠ 0。证明是完整的。</p><p id="eb38" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">同样的逻辑，如果q是Cᵗ的一个特征向量，那么Aᵗq就是c对应的特征向量，因为(αAᵗA)Aᵗq = Aᵗ(αAAᵗ)q = Aᵗλq = λ(Aᵗq).现在让<code class="du lf lg lh li b">x₁ … x₃₀₀₀</code>成为小得多的Cᵗ.的本征基根据<a class="ae lz" href="#8d80" rel="noopener ugc nofollow"> <em class="ly">引理2 </em> </a>，我们可以通过将<code class="du lf lg lh li b">x₁ … x₃₀₀₀</code>与Aᵗ.相乘，将它们转换回c中的特征向量设tᵢ是c中与Cᵗ的xᵢ共享相同特征值的特征向量，则:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/f6f2d40d9758e52848c5aec3449978d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/format:webp/1*5Chbsa1JpJ1pFSIRfnZ1Pw.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 12</figcaption></figure><p id="c336" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">结果，<code class="du lf lg lh li b">t₁ … t₃₀₀₀</code>形成了我们之前引入的子空间T，它捕获了与我们的原始人脸空间相同的方差。如果我们的目标是在不损失任何方差的情况下降低维数。我们可以停在这里，让T成为我们最终的特征脸子空间。我们称每个特征向量<code class="du lf lg lh li b">t₁ … t₃₀₀₀</code>为特征脸。</p><h2 id="8065" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">下一篇文章的总结和预览</h2><p id="8ede" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">在本文中，我们的目标是保持面部空间的整体方差。作为一种折衷，我们仍然需要包含变化相对较小的维度，这增加了计算的难度。如果我们愿意牺牲较小的方差损失来换取计算效率的较大提高，我们可以通过仅选择具有最大方差的维度来进一步降低特征脸子空间的维度。在下一篇文章中，我将介绍如何通过主成分分析(PCA)来实现这一点。和这篇文章一样，我将把重点放在PCA的数学证明上。为了结束这个系列，我还将讨论如何将原始学生照片投影到我们将通过PCA创建的特征脸子空间。</p></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="620a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">参考:</p><p id="c5be" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="ly">【1】j2kun。特征脸，用于面部识别。</em><a class="ae lz" href="https://jeremykun.com/2011/07/27/eigenfaces" rel="noopener ugc nofollow" target="_blank"><em class="ly"/></a><em class="ly">。</em></p><p id="f8d7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="ly">【2】彼得·奥尔弗和谢尔扎德·沙基班。应用线性代数。斯普林格，2006年</em></p><p id="bae8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="ly"> [3]马修·特克和亚历克斯·彭特兰。用于识别的特征脸。第71-86页。认知神经科学杂志，1991年。</em></p><p id="a880" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="ly">【4】封面照片记入【https://yutsumura.com/】<a class="ae lz" href="https://yutsumura.com/" rel="noopener ugc nofollow" target="_blank"><em class="ly"/></a></em></p></div></div>    
</body>
</html>