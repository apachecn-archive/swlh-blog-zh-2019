<html>
<head>
<title>Designing Linear Regression from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始设计线性回归</h1>
<blockquote>原文：<a href="https://medium.com/swlh/designing-linear-regression-from-scratch-ed542bf06943?source=collection_archive---------8-----------------------#2019-01-18">https://medium.com/swlh/designing-linear-regression-from-scratch-ed542bf06943?source=collection_archive---------8-----------------------#2019-01-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2cec26d90e83e5abde866b25c6738e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ozY-UOhVmjEZloLxDBKxVg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@antoine1003?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Antoine Dautry</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="a68f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">目标</h2><p id="33a7" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">在这本笔记本中，我们将尝试使用线性代数原理，以Python作为编程语言，从头开始构建线性回归算法</p><h2 id="3444" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">线性回归背后的思想</h2><p id="adcf" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">要开始理解机器学习算法如何在内部工作，线性回归通常是最符合逻辑和最直观的起点，因为它基于对基本线性代数的理解</p><p id="e3ee" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">在<strong class="jv kt">线性代数</strong>中，坐标系中的一条线由以下等式定义:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="076e" class="iv iw hi kz b fi ld le l lf lg">y = mx + c</span></pre><p id="bb1d" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">其中，<br/> m是系数(也称为直线的斜率或y随x变化的速率)<br/> c是截距，表示直线与y轴相交的点</p><p id="5bb8" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">给定m &amp; c，等式可用于找出对应于给定的一组x值的y值。<br/>另一种思考方式是，是否存在给定的这种值m &amp; c，可以在x &amp; y之间建立线性关系。如果存在，那么一个变量(y —因变量/响应变量)可以说与另一个变量(x —自变量/预测变量)线性相关</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/1de1b07943315f5166277fd9e575c5ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*9HOVUG3fbcibWTU9kM7_tg.jpeg"/></div></figure><p id="6d05" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">在<strong class="jv kt">机器学习</strong>中，同样的概念可以用来确定现实世界实体之间的关系。<br/>例如，给定特定年龄的一组人的属性“体重”&amp;“身高”，并且目标是找出他们的体重是否与他们的身高相关——在这里应用线性回归算法将有助于确定是否存在这样的值组合，即他们的身高与他们的体重线性相关</p><h2 id="97dc" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">工作原理:</h2><p id="0b50" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">线性回归算法从假设两个给定变量之间存在线性关系开始，这被称为初始<strong class="jv kt">假设</strong> <br/>算法然后试图找到系数‘m’&amp;的值，即截距‘c’。然而，对于真实世界的实体，绝对线性可能并不总是可能的-因此目标是以这样一种方式找到值，即x &amp; y之间的关系在最大可能的程度上是线性的，也就是说，产生的线应该尽可能接近所有给定的数据点。有不同的优化方法，但在本笔记本中，我们将探索普通的最小二乘法<br/>，计算值可用于预测响应变量(在<strong class="jv kt">预测分析</strong>)。通过这种方式，算法继续确定假设或初始假设是否正确——给定变量之间的线性关系是否(完全)或有多强</p><h1 id="ea09" class="li iw hi bd ix lj lk ll jb lm ln lo jf lp lq lr jj ls lt lu jn lv lw lx jr ly bi translated">设计算法</h1><h2 id="09b9" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">关于数据</h2><p id="2a6f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">在本练习中，我们将使用UCI数据集库中的葡萄酒质量数据集:<a class="ae iu" href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/Wine+Quality</a><br/>该数据集列出了影响葡萄牙“Vinho Verde”葡萄酒质量的不同属性的记录值。该分析将找出在目标变量“质量”的任何属性&amp;之间是否存在线性关系</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/dfdb68532cd5836c10dd83799f1d1779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dWRudSWsLp2DXnLgLZ39A.png"/></div></div></figure><p id="2012" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">这里，由于目标是从头开始构建算法，我们将只使用属性之一(<strong class="jv kt">单变量</strong>)作为预测变量(挥发性酸度)，&amp;尝试确定其与响应变量(葡萄酒质量)的关系</p><h2 id="ccd8" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">步骤:</h2><ul class=""><li id="f4ce" class="ma mb hi jv b jw jx ka kb jg mc jk md jo me kn mf mg mh mi bi translated">基于普通最小二乘准则确定系数和截距</li><li id="8bda" class="ma mb hi jv b jw mj ka mk jg ml jk mm jo mn kn mf mg mh mi bi translated">使用这些值来预测响应变量</li><li id="2493" class="ma mb hi jv b jw mj ka mk jg ml jk mm jo mn kn mf mg mh mi bi translated">使用R平方标准计算精度</li><li id="fe86" class="ma mb hi jv b jw mj ka mk jg ml jk mm jo mn kn mf mg mh mi bi translated">使用Python库中可用的线性回归模型实现相同的功能:Statsmodel &amp; Sci-kit learn</li><li id="0bd7" class="ma mb hi jv b jw mj ka mk jg ml jk mm jo mn kn mf mg mh mi bi translated">将此模型的准确性与Statsmodel &amp; Sci-kit learn实现的准确性进行比较</li></ul><h2 id="1be7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">使用OLS确定系数和截距</h2><p id="0bec" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated"><strong class="jv kt">普通最小二乘法</strong>是一种估计线性回归模型中未知参数的方法，目标是最小化结果线上的点与实际数据点之间的差值的平方和</p><p id="0aa9" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">按照这种方法，<br/>系数由公式给出:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/2a604167eb47cfc7ce55c157b199dcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Z4ZV1VwPwGu_YxQ22JzDqw.png"/></div></figure><p id="d528" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">截距由以下公式给出:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/dbdb349dcf92fc9acd48d61d755e2231.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*2YtQKBZqWzmulHlizMO60w.png"/></div></figure><p id="54fa" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">我们现在将在数据集上应用这个公式来确定系数(m)和截距</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="0f0a" class="iv iw hi kz b fi ld le l lf lg">xm=df["volatile_acidity"].mean()<br/>ym=df["quality"].mean()<br/>m = np.sum((df["volatile_acidity"] - xm) * (df["quality"] - ym))/np.sum(np.square(df["volatile_acidity"] - xm))<br/>c = ym - b1*xm</span></pre><p id="5a1b" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">运行上面的代码，我们得到了下面的结果:<br/>系数:-1.761437780112675 <br/>截距:6.2676676767</p><h2 id="b74c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">预测响应变量“质量”</h2><p id="a268" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">现在，使用这些系数和截距值来预测葡萄酒质量</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="2a17" class="iv iw hi kz b fi ld le l lf lg"># y=mx + c<br/>df["pred"] = m*df["volatile_acidity"] + c</span></pre><h2 id="456b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">计算精度(R平方度量)</h2><p id="781c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">r平方度量由以下公式给出:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/c1edb7ecc2e835345faa2aeb2237093d.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*FMc3_HgTDf9Q1ycUQVKqMw.png"/></div></figure><p id="cc96" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">其中，<br/> RSS=残差平方和<br/> TSS=总平方和</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/70074d8b56834caa132ee6930cf8f3ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*zLZjZumhCXYuW7rKcQGK7w.png"/></div></figure><p id="c937" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">其中，e[ith]= y[预测]—y[实际]，代表第I个残差，即第I个预测值与第I个观察响应之间的差值，以及</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/9e916ade5fb627eed7b1bcbc024dda85.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*iq-njfa3wrtNxi7xcesjFw.png"/></div></figure><p id="56d6" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">我们将使用上述公式来计算我们预测的R平方:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="5319" class="iv iw hi kz b fi ld le l lf lg">r2 = 1 - (np.sum(np.square(df["pred"] - df["quality"])) / np.sum(np.square(df["quality"] - ym)))</span></pre><p id="9a92" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">r的平方:0.153858686866</p><h1 id="de74" class="li iw hi bd ix lj lk ll jb lm ln lo jf lp lq lr jj ls lt lu jn lv lw lx jr ly bi translated">统计模型线性回归</h1><p id="5707" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">现在让我们使用Statsmodel &amp; Sci-kit learn实现同样的功能</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="7859" class="iv iw hi kz b fi ld le l lf lg">X = df[['volatile_acidity']]<br/>y=df[["quality"]]</span><span id="94ec" class="iv iw hi kz b fi mt le l lf lg">model = ols("""quality ~ volatile_acidity""", data=df)<br/>model=model.fit()<br/>predictions = model.predict(X)</span></pre><p id="a86b" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">模型摘要:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/165773a95bcb506c86e64299fd3a0030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*Sm9qj8A0xRAzKgXdh7ze_g.png"/></div></figure><p id="97be" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">系数:-1.761438 <br/>截距:6.565746 <br/> R的平方:0.15667686866</p><h1 id="f4bb" class="li iw hi bd ix lj lk ll jb lm ln lo jf lp lq lr jj ls lt lu jn lv lw lx jr ly bi translated">Sklearn线性回归</h1><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="dd85" class="iv iw hi kz b fi ld le l lf lg">lm = linear_model.LinearRegression()<br/>model1 = lm.fit(X,y)<br/>predictions1 = lm.predict(X)</span></pre><p id="61b1" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">系数:-1.76143778 <br/>截距:6.56574551 <br/> R的平方:0 . 4367586667</p><h1 id="ecd2" class="li iw hi bd ix lj lk ll jb lm ln lo jf lp lq lr jj ls lt lu jn lv lw lx jr ly bi translated">结论</h1><p id="ebfd" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">正如我们所看到的，所有的值在不同的实现中都是相同的:</p><ul class=""><li id="6b34" class="ma mb hi jv b jw ko ka kp jg mv jk mw jo mx kn mf mg mh mi bi translated">系数:-1.76</li><li id="f358" class="ma mb hi jv b jw mj ka mk jg ml jk mm jo mn kn mf mg mh mi bi translated">截距:6.565</li><li id="eac9" class="ma mb hi jv b jw mj ka mk jg ml jk mm jo mn kn mf mg mh mi bi translated">r平方值:0.152</li></ul><p id="a50c" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">这表明我们的线性回归的自我实现与库定义模型的实现是同步的<br/>。因此，我们可以断定<strong class="jv kt">已经从零开始成功地构建了一个线性回归模型</strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es my"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">这篇文章发表在<a class="ae iu" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有+412，714人关注。</h2><h2 id="869c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">在此订阅接收<a class="ae iu" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="ku kv kw kx fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es my"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>