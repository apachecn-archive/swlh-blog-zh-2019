<html>
<head>
<title>How to run Keras model inference x3 times faster with CPU and Intel OpenVINO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用CPU和英特尔OpenVINO将Keras模型推理运行速度提高x3倍</h1>
<blockquote>原文：<a href="https://medium.com/swlh/how-to-run-keras-model-inference-x3-times-faster-with-cpu-and-intel-openvino-85aa10099d27#2019-01-28">https://medium.com/swlh/how-to-run-keras-model-inference-x3-times-faster-with-cpu-and-intel-openvino-85aa10099d27#2019-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/10f43d273693d5ec4a17b934d5c9b01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T044a5_tPQz6KrdU.png"/></div></div></figure><p id="6c2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个快速教程中，您将学习如何设置OpenVINO，并在不增加任何硬件的情况下使您的Keras模型推断速度至少提高3倍。</p><p id="cf57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然有多种选择可以加速你在边缘设备上的深度学习推理，举几个例子，</p><ol class=""><li id="33ea" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">添加低端Nvidia GPU，如GT1030</li></ol><ul class=""><li id="912d" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">优点:</li></ul><blockquote class="jy jz ka"><p id="c001" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">易于集成，因为它也利用Nvidia的CUDA和CuDNN工具包来加速推理，就像您的开发环境一样，不需要进行重大的模型转换。</p></blockquote><ul class=""><li id="a0e3" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">缺点:</li></ul><blockquote class="jy jz ka"><p id="6a0f" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">PCI-E插槽必须存在于目标设备的主板上，以与图形卡接口，这增加了边缘设备的额外成本和空间。</p></blockquote><p id="fdd8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.使用面向加速神经网络推理的ASIC芯片，如<a class="ae kf" href="https://software.intel.com/en-us/movidius-ncs" rel="noopener ugc nofollow" target="_blank"> Movidius神经计算棒</a>、<a class="ae kf" href="https://www.gyrfalcontech.ai/solutions/2801s/" rel="noopener ugc nofollow" target="_blank"> Lightspeeur 2801神经加速器</a>。</p><ul class=""><li id="7228" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">优点:</li></ul><blockquote class="jy jz ka"><p id="cd83" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">就像u盘一样，它们也工作在不同的主机上，无论是采用Intel/AMD CPU的台式电脑，还是采用ARM Cortex-A的Raspberry Pi单板电脑。</p><p id="7084" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">神经网络计算被卸载到这些u盘上，使得主机的CPU只需担心更多的通用计算，如图像预处理。</p><p id="875c" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">随着边缘设备上吞吐量需求的增加，扩展可以像插入更多USB棒一样简单。</p><p id="5da5" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">与CPU/Nvidia GPU相比，它们通常具有更高的性能功耗比。</p></blockquote><ul class=""><li id="7a8e" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">缺点:</li></ul><blockquote class="jy jz ka"><p id="c296" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">由于它们是ASIC(专用集成电路),预计对某些TensorFlow层/操作的支持有限。</p><p id="d33a" class="iq ir kb is b it iu iv iw ix iy iz ja kc jc jd je kd jg jh ji ke jk jl jm jn hb bi translated">它们还需要特殊的模型转换来创建特定ASIC可理解的指令。</p></blockquote><p id="bba5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.嵌入式SoC带有NPU(神经处理单元)，如Rockchip RK3399Pro。</p><ul class=""><li id="7ed7" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">NPU类似于ASIC芯片，需要特殊的指令和模型转换。不同之处在于，它们与CPU位于同一个硅芯片中，这使得外形尺寸更小。</li></ul><p id="ad54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">前面提到的所有加速选项都需要额外的成本。但是，如果一个边缘设备已经有了英特尔的CPU，你还不如用英特尔的OpenVINO toolkit免费加速它的深度学习推理速度x3倍。</p><h1 id="13c3" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">OpenVINO和设置简介</h1><p id="5c3f" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">您可能想知道，如果没有额外的硬件，额外的加速从何而来？</p><p id="bac5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，因为OpenVINO是英特尔的产品，所以它针对其处理器进行了优化。</p><p id="a351" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">OpenVINO推理引擎可以推理具有不同输入精度支持的CPU或英特尔集成GPU模型。</p><p id="b858" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CPU只支持FP32，而它的GPU同时支持FP16和FP32。</p><p id="b878" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CPU插件利用面向深度神经网络(MKL-DNN)的英特尔数学内核库以及OpenMP来并行化计算。</p><p id="1351" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您将在本教程的后面部分看到模型优化，在此过程中，会采取额外的步骤来使模型更加紧凑，以便进行推理。</p><ul class=""><li id="f48e" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">群卷积的合并。</li><li id="5cd9" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">用ReLU或eLU融合卷积。</li><li id="a621" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">融合卷积+和或卷积+和+ ReLu。</li><li id="8c5c" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">移除电源层。</li></ul><p id="2193" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们在你的机器上安装OpenVINO，在<a class="ae kf" href="https://software.intel.com/en-us/openvino-toolkit/choose-download" rel="noopener ugc nofollow" target="_blank">这个页面</a>上选择你的操作系统，按照说明下载并安装它。</p><p id="d8d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is lo">系统要求</strong></p><ul class=""><li id="1a50" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">第六至第八代英特尔酷睿</li><li id="3479" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">英特尔至强v5家族</li><li id="6f21" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">英特尔至强v6家族</li></ul><p id="18ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is lo">操作系统</strong></p><ul class=""><li id="ac50" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">Ubuntu* 16.04.3长期支持(LTS)，64位</li><li id="aca9" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">CentOS* 7.4，64位</li><li id="a2a2" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">64位Windows* 10</li></ul><p id="7a99" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您已经安装了Python 3.5+，则可以安全地忽略安装Python 3.6+的通知。</p><p id="0498" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">安装完成后，运行<code class="du lp lq lr ls b">C:/Intel/computer_vision_sdk/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites_tf.bat</code></p><p id="dc17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">或者<code class="du lp lq lr ls b">~/Intel/computer_vision_sdk/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites_tf.sh</code></p><p id="bf71" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">依赖于您的操作系统来安装OpenVINO使用TensorFlow所需的任何Python包。</p><h1 id="739b" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">OpenVINO中的InceptionV3模型推理</h1><p id="5190" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">您可以从<a class="ae kf" href="https://github.com/Tony607/keras_openvino" rel="noopener ugc nofollow" target="_blank"> my GitHub </a>下载本教程的完整源代码，它包括一个all in one Jupyter笔记本，引导您为OpenVINO转换Keras模型，进行预测以及对所有三种环境——Keras、TensorFlow和open vino——的推理速度进行基准测试。</p><p id="4721" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在调用<code class="du lp lq lr ls b">jupyter notebook</code>之前运行<code class="du lp lq lr ls b">setupvars.bat</code>来设置环境。</p><pre class="lt lu lv lw fd lx ls ly lz aw ma bi"><span id="cbfa" class="mb kh hi ls b fi mc md l me mf">C:\Intel\computer_vision_sdk\bin\setupvars.bat</span></pre><p id="981a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">或者在Linux中添加下面一行到<code class="du lp lq lr ls b">~/.bashrc</code></p><pre class="lt lu lv lw fd lx ls ly lz aw ma bi"><span id="2b16" class="mb kh hi ls b fi mc md l me mf">source ~/intel/computer_vision_sdk/bin/setupvars.sh</span></pre><p id="2980" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是将Keras模型转换为OpenVINO模型并进行预测的工作流程概述。</p><ol class=""><li id="d4d6" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">将Keras模型保存为单个<code class="du lp lq lr ls b">.h5</code>文件。</li><li id="87be" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jt ju jv jw bi translated">加载<code class="du lp lq lr ls b">.h5</code>文件并将图形冻结到一个TensorFlow <code class="du lp lq lr ls b">.pb</code>文件中。</li><li id="7dc2" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jt ju jv jw bi translated">运行OpenVINO <code class="du lp lq lr ls b">mo_tf.py</code>脚本，将<code class="du lp lq lr ls b">.pb</code>文件转换为模型XML和bin文件。</li><li id="da9c" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jt ju jv jw bi translated">用OpenVINO推理引擎加载模型XML和bin文件，并进行预测。</li></ol><h1 id="18df" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">将Keras模型保存为单个<code class="du lp lq lr ls b">.h5</code>文件</h1><p id="f376" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">对于本教程，我们将从Keras加载一个预训练的ImageNet分类InceptionV3模型，</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h1 id="5fdf" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">将图形冻结为单个张量流<code class="du lp lq lr ls b">.pb</code>文件</h1><p id="27f1" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">这一步将删除推理不需要的任何层和操作。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h1 id="6a23" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">OpenVINO模型优化</h1><p id="f7b7" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">以下代码片段在Jupyter笔记本中运行，它根据您的操作系统(Windows或Linux)定位<code class="du lp lq lr ls b">mo_tf.py</code>脚本，您可以相应地更改<code class="du lp lq lr ls b">img_height</code>。<code class="du lp lq lr ls b">data_type</code>也可以设置为FP16，以便在英特尔集成GPU上进行推理时获得额外的速度提升，同时降低进动。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="3b3e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">运行脚本后，您会发现在目录<code class="du lp lq lr ls b">./model</code>、<code class="du lp lq lr ls b">frozen_model.xml</code>和<code class="du lp lq lr ls b">frozen_model.bin</code>下生成了两个新文件。它们是基于训练好的网络拓扑、权重和偏差值的模型的优化中间表示(IR)。</p><h1 id="c7c9" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">使用OpenVINO推理引擎(IE)进行推理</h1><p id="4f7d" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">如果您已经正确设置了环境，那么像<code class="du lp lq lr ls b">C:\Intel\computer_vision_sdk\python\python3.5</code>或<code class="du lp lq lr ls b">~/intel/computer_vision_sdk/python/python3.5</code>这样的路径将存在于<code class="du lp lq lr ls b">PYTHONPATH</code>中。这是在运行时加载Python <code class="du lp lq lr ls b">openvino</code>包所必需的。</p><p id="62c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下代码片段使用CPU运行推理引擎，如果您之前选择使用FP16 <code class="du lp lq lr ls b">data_type</code>，它也可以在英特尔GPU上运行。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h1 id="ae8a" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">速度基准</h1><p id="d3e2" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">基准设置，</p><ul class=""><li id="b42a" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">TensorFlow版本:1.12.0</li><li id="eea2" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">操作系统:Windows 10，64位</li><li id="714e" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">CPU:英特尔酷睿i7–7700 HQ</li><li id="a58f" class="jo jp hi is b it lj ix lk jb ll jf lm jj ln jn jx ju jv jw bi translated">计算平均结果的推理次数:20。</li></ul><p id="a638" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有三种环境的基准测试结果— Keras、TensorFlow和OpenVINO如下所示。</p><pre class="lt lu lv lw fd lx ls ly lz aw ma bi"><span id="6d0f" class="mb kh hi ls b fi mc md l me mf">Keras          average(sec):0.079, fps:12.5<br/>TensorFlow     average(sec):0.069, fps:14.3<br/>OpenVINO(CPU)  average(sec):0.024, fps:40.6</span></pre><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/86cacae6b6b423549616490b0aac154a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*ohKvzVyWeJ50WKSkvckXQA.png"/></div></figure><p id="3063" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结果可能因您正在试验的英特尔处理器而异，但与在CPU后端使用TensorFlow / Keras运行推理相比，预计会有显著的加速。</p><h1 id="4980" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论和进一步阅读</h1><p id="9371" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">在本教程中，您学习了如何使用英特尔处理器和OpenVINO toolkit运行模型推理，其速度比股票TensorFlow快几倍。虽然OpenVINO不仅可以加速CPU上的推理，但本教程中介绍的相同工作流可以很容易地适应Movidius neural compute stick，只需进行一些更改。</p><p id="786b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">OpenVINO文档可能对您有所帮助。</p><p id="9f7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae kf" href="https://software.intel.com/en-us/articles/OpenVINO-Install-Windows" rel="noopener ugc nofollow" target="_blank">安装英特尔open vino toolkit for Windows * 10分发版</a></p><p id="2e7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae kf" href="https://software.intel.com/en-us/articles/OpenVINO-Install-Linux" rel="noopener ugc nofollow" target="_blank">安装英特尔发布的open vino toolkit for Linux *</a></p><p id="b8d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae kf" href="https://software.intel.com/en-us/articles/OpenVINO-InferEngine#inpage-nav-8-2-2" rel="noopener ugc nofollow" target="_blank">open vino——高级主题——CPU插件</a>,在这里您可以了解更多关于各种模型优化技术的信息。</p><h2 id="a4d3" class="mb kh hi bd ki mj mk ml km mm mn mo kq jb mp mq ku jf mr ms ky jj mt mu lc mv bi translated">从<a class="ae kf" href="https://github.com/Tony607/keras_openvino" rel="noopener ugc nofollow" target="_blank"> my GitHub </a>下载本教程的完整源代码。</h2><p id="b569" class="pw-post-body-paragraph iq ir hi is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated"><a class="ae kf" href="https://twitter.com/intent/tweet?url=https%3A//www.dlology.com/blog/how-to-run-keras-model-inference-x3-times-faster-with-cpu-and-intel-openvino-1/&amp;text=How%20to%20run%20Keras%20model%20inference%20x3%20times%20faster%20with%20CPU%20and%20Intel%20OpenVINO" rel="noopener ugc nofollow" target="_blank">在推特上分享</a> <a class="ae kf" href="https://www.facebook.com/sharer/sharer.php?u=https://www.dlology.com/blog/how-to-run-keras-model-inference-x3-times-faster-with-cpu-and-intel-openvino-1/" rel="noopener ugc nofollow" target="_blank">在脸书分享</a></p></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><p id="8fc9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kb">原载于</em><a class="ae kf" href="https://www.dlology.com/blog/how-to-run-keras-model-inference-x3-times-faster-with-cpu-and-intel-openvino-1/" rel="noopener ugc nofollow" target="_blank"><em class="kb">www.dlology.com</em></a><em class="kb">。</em></p><figure class="lt lu lv lw fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es nd"><img src="../Images/308a8d84fb9b2fab43d66c117fcc4bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqDjlKFwScoQYQ62DWEdig.png"/></div></a></figure><h2 id="c148" class="mb kh hi bd ki mj mk ml km mm mn mo kq jb mp mq ku jf mr ms ky jj mt mu lc mv bi translated">这篇文章发表在<a class="ae kf" href="https://medium.com/swlh" rel="noopener"> The Startup </a>上，这是Medium最大的创业刊物，拥有+416，678名读者。</h2><h2 id="869c" class="mb kh hi bd ki mj mk ml km mm mn mo kq jb mp mq ku jf mr ms ky jj mt mu lc mv bi translated">在这里订阅接收<a class="ae kf" href="http://growthsupply.com/the-startup-newsletter/" rel="noopener ugc nofollow" target="_blank">我们的头条新闻</a>。</h2><figure class="lt lu lv lw fd ij er es paragraph-image"><a href="https://medium.com/swlh"><div class="er es nd"><img src="../Images/b0164736ea17a63403e660de5dedf91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouK9XR4xuNWtCes-TIUNAw.png"/></div></a></figure></div></div>    
</body>
</html>